{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Building a seq2seq model for machine translation.**"
      ],
      "metadata": {
        "id": "rBeCI8XAPcoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Language Choice and Details**\n",
        "\n",
        "I initially wanted to build a model to translate English to **Tamil** which happens to be my Native Language so that I could work with the translations easily. But due to less number of available sentence pairs (207) on the https://www.manythings.org/anki/ website for this language, I picked **Hebrew**.\n",
        "\n",
        "Though Hebrew has been a language of fascination for me for a while now, there are a few other important reasons I picked Hebrew for learning machine translation.\n",
        "\n",
        "- Hebrew is **written from right to left**, it will be interesting to see if Bi-LSTM and Attention produces better results.\n",
        "\n",
        "- English and Hebrew are from completely different language-families and roots. (https://webspace.ship.edu/cgboer/languagefamilies.html)\n",
        "They are from completely different regions and time-periods and have isolated places of origins.\n",
        "  - English -> **Region:** West Germany, **Language family:** Indo-European, **Root:** Germanic\n",
        "  - Hebrew -> **Region:** Israel, **Language family:** Afro-Asiatic, **Root:** Semitic\n",
        "\n",
        "- They share almost a near-zero lexical similarity (https://en.wikipedia.org/wiki/Lexical_similarity) and less genetic relationship and linguistic interference. (https://en.wikipedia.org/wiki/Genetic_relationship_(linguistics))\n",
        "\n",
        "- English-Hebrew also has a good number of sentence pairs available on the https://www.manythings.org/anki/ website(127856)\n",
        "\n"
      ],
      "metadata": {
        "id": "ypW348xvhSIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation\n",
        "\n",
        "heb.txt under home directory"
      ],
      "metadata": {
        "id": "rhPDH1G9P1Xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and clean text\n"
      ],
      "metadata": {
        "id": "sgeZcXIaP2xA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Since canonical normalization (normalize-NFD) does not work with Hebrew characters, I'm using an open-source library for tokenizing Hebrew text (https://github.com/YontiLevin/Hebrew-Tokenizer)**"
      ],
      "metadata": {
        "id": "EYWVvGhC0N42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install hebrew_tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXCXLMOkOWwh",
        "outputId": "f2fb4760-b49c-4e36-aec8-1176caec0edf"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hebrew_tokenizer in /usr/local/lib/python3.10/dist-packages (2.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hebrew tokenizer by github user 'YontiLevin'\n",
        "import hebrew_tokenizer as ht"
      ],
      "metadata": {
        "id": "K9YYuAbMOb9w"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "from unicodedata import normalize\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "    lines = doc.strip().split('\\n')\n",
        "    pairs = [line.split('\\t') for line in  lines]\n",
        "    return pairs\n",
        "\n",
        "def is_hebrew(text):\n",
        "    # Check if the text contains Hebrew characters\n",
        "    return any('\\u0590' <= char <= '\\u05EA' for char in text)\n",
        "\n",
        "def clean_data(lines):\n",
        "    cleaned = []  # Initialize an empty list for storing cleaned data\n",
        "\n",
        "    # Prepare regex for character filtering\n",
        "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "    # Prepare translation table for removing punctuation\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "    for pair in lines:\n",
        "        clean_pair = []\n",
        "\n",
        "        for line in pair:\n",
        "            # separate processing for English and Hebrew\n",
        "            if is_hebrew(line):\n",
        "                clean_tokens = []\n",
        "                # Tokenize the Hebrew line\n",
        "                tokens = ht.tokenize(line)\n",
        "\n",
        "                for grp, token, token_num, (start_index, end_index) in tokens:\n",
        "                  clean_tokens.append(('{}'.format(token)))\n",
        "\n",
        "                # Convert to lowercase\n",
        "                tokens = [word.lower() for word in tokens]\n",
        "\n",
        "                # Remove punctuation from each word\n",
        "                clean_tokens = [remove_punctuation_from_word(word, table) for word in clean_tokens]\n",
        "\n",
        "                # Remove non-printable characters from each token\n",
        "                #clean_tokens = [re_print.sub('', w) for w in tokens]\n",
        "\n",
        "                # Remove tokens with numbers in them\n",
        "                clean_tokens = [word for word in clean_tokens if word.isalpha()]\n",
        "            else:\n",
        "                # Normalize Unicode characters\n",
        "                line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "                line = line.decode('UTF-8')\n",
        "\n",
        "                # Tokenize on whitespace\n",
        "                tokens = line.split()\n",
        "\n",
        "                # Convert to lowercase\n",
        "                tokens = [word.lower() for word in tokens]\n",
        "\n",
        "                # Remove punctuation from each word\n",
        "                clean_tokens = [remove_punctuation_from_word(word, table) for word in tokens]\n",
        "\n",
        "                # Remove non-printable characters from each token\n",
        "                clean_tokens = [re_print.sub('', w) for w in clean_tokens]\n",
        "\n",
        "                # Remove tokens with numbers in them\n",
        "                clean_tokens = [word for word in clean_tokens if word.isalpha()]\n",
        "\n",
        "            # Store as a string\n",
        "            clean_line = ' '.join(clean_tokens)\n",
        "            clean_pair.append(clean_line)\n",
        "\n",
        "        cleaned.append(clean_pair)\n",
        "\n",
        "    return np.array(cleaned)\n",
        "\n",
        "def remove_punctuation_from_word(word, table):\n",
        "    return word.translate(table)\n"
      ],
      "metadata": {
        "id": "6_vfLbK_R5xa"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "JR2LZrdkPYJB"
      },
      "outputs": [],
      "source": [
        "# filename\n",
        "filename = 'heb.txt'\n",
        "\n",
        "#number of sentences for training\n",
        "#n_train = 100000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **After cleaning I randomly split the data to 80:20 for train and test** (validation data is 20% within training set and is used for tuning the model)\n",
        "**Optional:** Ignore last few sentence pairs for faster training as they are huge and model requires enormous resources and a lot of time to train after OHE"
      ],
      "metadata": {
        "id": "u-LOIZwTnBLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "doc = load_doc(filename)\n",
        "\n",
        "# split into Language1-Language2 pairs\n",
        "pairs = to_pairs(doc)\n",
        "\n",
        "# Clean sentences\n",
        "clean= clean_data(pairs)\n",
        "\n",
        "# optional - ignore high length sentences (if RAM is low)\n",
        "clean = clean[0: 35000]\n",
        "\n",
        "# Shuffle the pairs randomly\n",
        "np.random.shuffle(clean)\n",
        "\n",
        "# Calculate the split index for 80% training and 20% test\n",
        "split_index = int(0.8 * len(clean))\n",
        "\n",
        "# Split the pairs into training and test sets\n",
        "clean_pairs = clean[:split_index]\n",
        "clean_pairs_test = clean[split_index:]\n",
        "\n",
        "# Print the number of pairs in each set\n",
        "print(\"Number of training pairs:\", len(clean_pairs))\n",
        "print(\"Number of test pairs:\", len(clean_pairs_test))"
      ],
      "metadata": {
        "id": "YKhNfurGRAu_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98127483-34b7-4d50-eeab-519ef1163746"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training pairs: 28000\n",
            "Number of test pairs: 7000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print sample - not cleaned\n",
        "print(pairs[5000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gngz5k7VmQo8",
        "outputId": "8f85d4a1-375d-40af-9437-6e3dae303b70"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"They're fine.\", 'הם בסדר.', 'CC-BY 2.0 (France) Attribution: tatoeba.org #2111291 (CK) & #5426571 (fekundulo)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training set"
      ],
      "metadata": {
        "id": "YXyRASSAsD1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print samples after cleaning - training\n",
        "print(clean_pairs[5000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuMFGsIomYNe",
        "outputId": "3a4a0d7e-b962-4177-cf45-1cfc89698a4b"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i have to dress up' 'אני צריכה להתלבש'\n",
            " 'ccby france attribution tatoebaorg ck nava']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5000, 5010):\n",
        "    print('[' + clean_pairs[i][0] + '] => [' + clean_pairs[i][1] + ']')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRHn9lObREck",
        "outputId": "4ced1c47-c828-4c36-9efe-c970ba38e41c"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[i have to dress up] => [אני צריכה להתלבש]\n",
            "[he appeared young] => [הוא נראה צעיר]\n",
            "[its a catchy song] => [זה שיר קליט]\n",
            "[what dont you have] => [מה אין לך]\n",
            "[look at the sky] => [תסתכלי על השמיים]\n",
            "[i did it for you] => [עשיתי את זה בשבילך]\n",
            "[could that change] => [זה יכול להשתנות]\n",
            "[you cheated] => [רמית]\n",
            "[ill be in the car] => [אהיה במכונית]\n",
            "[long time no see] => [מזמן לא התראינו]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts = clean_pairs[:, 0]\n",
        "target_texts = ['\\t' + text + '\\n' for text in clean_pairs[:, 1]]\n",
        "\n",
        "print('Length of input_texts:  ' + str(input_texts.shape))\n",
        "print('Length of target_texts: ' + str(input_texts.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KMDpsxYUIAZ",
        "outputId": "b747ee7b-2eb8-473a-e4d5-636da42cf89f"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of input_texts:  (28000,)\n",
            "Length of target_texts: (28000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_texts[5000])\n",
        "print(target_texts[5000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Cp4c9eyVbk5",
        "outputId": "fd0d7278-5c69-4158-c5fa-7a3b8c855d75"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i have to dress up\n",
            "\tאני צריכה להתלבש\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# highest length of inputs and outputs\n",
        "max_encoder_seq_length = max(len(line) for line in input_texts)\n",
        "max_decoder_seq_length = max(len(line) for line in target_texts)\n",
        "\n",
        "print('max length of input  sentences: %d' % (max_encoder_seq_length))\n",
        "print('max length of target sentences: %d' % (max_decoder_seq_length))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PsiM19uUJN6",
        "outputId": "fee5b279-234c-4538-c630-c2a4f2ac50d7"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max length of input  sentences: 20\n",
            "max length of target sentences: 44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test set"
      ],
      "metadata": {
        "id": "4gQfI9kwsGGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print samples after cleaning - test\n",
        "print(clean_pairs_test[5000])"
      ],
      "metadata": {
        "id": "VMfq-7MHsHrA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dfc841a-cba4-4287-b5b5-7212b4b5c0fc"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tom always lies' 'תום תמיד משקר'\n",
            " 'ccby france attribution tatoebaorg joseph fekundulo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5000, 5010):\n",
        "    print('[' + clean_pairs_test[i][0] + '] => [' + clean_pairs_test[i][1] + ']')"
      ],
      "metadata": {
        "id": "bevzLGaJsITP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "177af4a0-f92e-4848-dad3-f129168fc648"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tom always lies] => [תום תמיד משקר]\n",
            "[can you go get it] => [אתה יכול להביא את זה]\n",
            "[i like them] => [אני אוהב אותם]\n",
            "[i never worry] => [לעולם אינני דואג]\n",
            "[were not ready] => [אנו לא מוכנות]\n",
            "[he and i are cousins] => [הוא ואני בני דודים]\n",
            "[you are so stupid] => [אתה כזה טמבל]\n",
            "[does tom know why] => [טום יודע למה]\n",
            "[they got nothing] => [הם לא קיבלו דבר]\n",
            "[i didnt order it] => [לא הזמנתי את זה]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_texts = clean_pairs_test[:, 0]\n",
        "test_target_texts = ['\\t' + text + '\\n' for text in clean_pairs_test[:, 1]]\n",
        "\n",
        "print('Length of input_texts:  ' + str(test_input_texts.shape))\n",
        "print('Length of target_texts: ' + str(test_input_texts.shape))"
      ],
      "metadata": {
        "id": "8HJ_TxEusPCZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3e56562-5bfc-4223-bfdf-e985c4a72abf"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of input_texts:  (7000,)\n",
            "Length of target_texts: (7000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text processing"
      ],
      "metadata": {
        "id": "cF86hr3QioSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert texts to sequences"
      ],
      "metadata": {
        "id": "AfYmJ0SLim5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Strategy:** If language is Hebrew, I tried adding padding to the left instead (since Hebrew writing is from right to left) - Used in model C (results were not good)\n",
        "\n",
        "##### **Strategy** : Reverse the token sequence for right to left learning in Hebrew"
      ],
      "metadata": {
        "id": "Jh4UNRQjHaqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "# encode and pad sequences\n",
        "\n",
        "def text2sequences_heb(max_len, lines, lang):\n",
        "\n",
        "    tokenizer = Tokenizer(char_level=True,filters='')\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    seqs = tokenizer.texts_to_sequences(lines)\n",
        "\n",
        "    if lang == \"Heb\":\n",
        "      seqs.reverse()\n",
        "    #    seqs_pad = pad_sequences(seqs, maxlen=max_len, padding='pre')\n",
        "    #else:\n",
        "    #    seqs_pad = pad_sequences(seqs, maxlen=max_len, padding='post')\n",
        "\n",
        "    seqs_pad = pad_sequences(seqs, maxlen=max_len, padding='post')\n",
        "\n",
        "    return seqs_pad, tokenizer.word_index"
      ],
      "metadata": {
        "id": "Nw_qctINje-G"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### text->seq : Training set"
      ],
      "metadata": {
        "id": "Wlq2Zicsruih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_seq, input_token_index = text2sequences_heb(max_encoder_seq_length,\n",
        "                                                      input_texts, lang=\"Eng\")\n",
        "decoder_input_seq, target_token_index = text2sequences_heb(max_decoder_seq_length,\n",
        "                                                       target_texts, lang=\"Heb\")\n",
        "\n",
        "\n",
        "print('shape of encoder_input_seq: ' + str(encoder_input_seq.shape))\n",
        "print('shape of input_token_index: ' + str(len(input_token_index)))\n",
        "\n",
        "print('shape of decoder_input_seq: ' + str(decoder_input_seq.shape))\n",
        "print('shape of target_token_index: ' + str(len(target_token_index)))"
      ],
      "metadata": {
        "id": "cZ8-jHS6rg42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12903af3-80a4-40c8-8168-a5a995e91e11"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of encoder_input_seq: (28000, 20)\n",
            "shape of input_token_index: 27\n",
            "shape of decoder_input_seq: (28000, 44)\n",
            "shape of target_token_index: 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoder_input_seq[5000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm5aCtr2ZP3J",
        "outputId": "fb6ee276-241d-47b6-b8b8-379f50287410"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 5  1 10  6 23  2  1  3  4  1 12  9  2  7  7  1 14 19  0  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_token_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1vF6doeKWD2",
        "outputId": "852c5b7c-7952-432e-8ea5-04eac7f1f622"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 1, 'e': 2, 't': 3, 'o': 4, 'i': 5, 'a': 6, 's': 7, 'n': 8, 'r': 9, 'h': 10, 'l': 11, 'd': 12, 'm': 13, 'u': 14, 'y': 15, 'w': 16, 'c': 17, 'g': 18, 'p': 19, 'k': 20, 'b': 21, 'f': 22, 'v': 23, 'j': 24, 'x': 25, 'q': 26, 'z': 27}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(target_token_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9ow6AvBKbq1",
        "outputId": "a60e93cd-e1ee-4074-9528-0d0e0ab08703"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 1, 'י': 2, 'ו': 3, 'ה': 4, '\\t': 5, '\\n': 6, 'א': 7, 'ת': 8, 'ל': 9, 'נ': 10, 'ר': 11, 'מ': 12, 'ם': 13, 'ב': 14, 'ש': 15, 'ע': 16, 'כ': 17, 'ח': 18, 'ז': 19, 'ד': 20, 'ק': 21, 'פ': 22, 'ס': 23, 'צ': 24, 'ג': 25, 'ט': 26, 'ך': 27, 'ן': 28, 'ף': 29, 'ץ': 30, 'i': 31, 'b': 32, 'f': 33, 'a': 34, 'r': 35, 'm': 36, 'w': 37, 'c': 38, 'n': 39, 'd': 40, 'v': 41, 'l': 42, 'o': 43, 'y': 44, 'k': 45, 'e': 46}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_encoder_tokens = len(input_token_index) + 1\n",
        "num_decoder_tokens = len(target_token_index) + 1\n",
        "\n",
        "print('num_encoder_tokens: ' + str(num_encoder_tokens))\n",
        "print('num_decoder_tokens: ' + str(num_decoder_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3uradX8jN_m",
        "outputId": "a8766a35-0426-4e0a-bdf4-92ced702cdc3"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_encoder_tokens: 28\n",
            "num_decoder_tokens: 47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts[5000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Fxf-lyM8IHhx",
        "outputId": "bd01542e-9983-49d3-d8f0-053cd352d096"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i have to dress up'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_texts[5000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8yOY2DbDjmxe",
        "outputId": "7bdfa14d-8cbc-4725-e2a5-a7896934e2b7"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\tאני צריכה להתלבש\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input_seq[5000, :]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QZ-Qu8Wjo9N",
        "outputId": "004bf141-6d1e-4a33-9042-4d1ee6c0b732"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5,  7,  9,  1,  8, 17,  7,  2, 14,  2,  1,  9,  2,  1, 14, 14, 21,\n",
              "       15,  4,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### text->seq : Test set"
      ],
      "metadata": {
        "id": "8aVeoty1r381"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_encoder_input_seq, test_input_token_index = text2sequences_heb(max_encoder_seq_length,\n",
        "                                                      test_input_texts, lang=\"Eng\")\n",
        "test_decoder_input_seq, test_target_token_index = text2sequences_heb(max_decoder_seq_length,\n",
        "                                                       test_target_texts, lang=\"Heb\")\n",
        "\n",
        "\n",
        "print('shape of encoder_input_seq: ' + str(test_encoder_input_seq.shape))\n",
        "print('shape of input_token_index: ' + str(len(test_input_token_index)))\n",
        "\n",
        "print('shape of decoder_input_seq: ' + str(test_decoder_input_seq.shape))\n",
        "print('shape of target_token_index: ' + str(len(test_target_token_index)))"
      ],
      "metadata": {
        "id": "jGRIPfwVsc7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5c23c7d-bdae-4341-8a4d-4627b7073753"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of encoder_input_seq: (7000, 20)\n",
            "shape of input_token_index: 27\n",
            "shape of decoder_input_seq: (7000, 44)\n",
            "shape of target_token_index: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-hot encode\n",
        "(Replace with generator)\n"
      ],
      "metadata": {
        "id": "5PEDoa55XdkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "import numpy\n",
        "# one hot encode target sequence\n",
        "def onehot_encode(sequences, max_len, vocab_size):\n",
        "    n = len(sequences)\n",
        "    data = numpy.zeros((n, max_len, vocab_size))\n",
        "    for i in range(n):\n",
        "        data[i, :, :] = to_categorical(sequences[i], num_classes=vocab_size)\n",
        "    return data"
      ],
      "metadata": {
        "id": "-IbsTbWJXYAy"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OHE - Training set"
      ],
      "metadata": {
        "id": "ysAVeFZvsi9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_data = onehot_encode(encoder_input_seq, max_encoder_seq_length, num_encoder_tokens)\n",
        "decoder_input_data = onehot_encode(decoder_input_seq, max_decoder_seq_length, num_decoder_tokens)\n",
        "\n",
        "decoder_target_seq = numpy.zeros(decoder_input_seq.shape)\n",
        "decoder_target_seq[:, 0:-1] = decoder_input_seq[:, 1:]\n",
        "decoder_target_data = onehot_encode(decoder_target_seq,\n",
        "                                    max_decoder_seq_length,\n",
        "                                    num_decoder_tokens)\n",
        "\n",
        "print(encoder_input_data.shape)\n",
        "print(decoder_input_data.shape)"
      ],
      "metadata": {
        "id": "qau9udEgspvr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f34e6fa-f6e2-4365-aba3-2bd8be60e92b"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28000, 20, 28)\n",
            "(28000, 44, 47)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OHE - Test set"
      ],
      "metadata": {
        "id": "BkAhL32wsqdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_encoder_input_data = onehot_encode(test_encoder_input_seq, max_encoder_seq_length, num_encoder_tokens)\n",
        "test_decoder_input_data = onehot_encode(test_decoder_input_seq, max_decoder_seq_length, num_decoder_tokens)\n",
        "\n",
        "test_decoder_target_seq = numpy.zeros(test_decoder_input_seq.shape)\n",
        "test_decoder_target_seq[:, 0:-1] = test_decoder_input_seq[:, 1:]\n",
        "test_decoder_target_data = onehot_encode(test_decoder_target_seq,\n",
        "                                    max_decoder_seq_length,\n",
        "                                    num_decoder_tokens)\n",
        "\n",
        "print(test_encoder_input_data.shape)\n",
        "print(test_decoder_input_data.shape)"
      ],
      "metadata": {
        "id": "3lJ0a43Esu_E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f5c86d3-7db0-459c-b11d-917c0015c86d"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7000, 20, 28)\n",
            "(7000, 44, 47)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Networks, Training and Prediction"
      ],
      "metadata": {
        "id": "TgyO0Y4Vodrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a) LSTM Seq2Seq model\n",
        "\n",
        "Combinations tried:\n",
        "\n",
        "- Latent dimensions - 128/256/512\n",
        "- training epochs - 10, 25, 50\n",
        "- dropout = 0.2, 0.5\n",
        "- activation function - RMS, Adam\n",
        "- sampling - greedy, multinomial (different temperatures)"
      ],
      "metadata": {
        "id": "D4jUuzgFXczh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### a1) Encoder network"
      ],
      "metadata": {
        "id": "ahKO7uq3XtuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, LSTM\n",
        "from keras.models import Model\n",
        "\n",
        "latent_dim = 256\n",
        "\n",
        "# inputs of the encoder network\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens),\n",
        "                       name='encoder_inputs')\n",
        "\n",
        "# set the LSTM layer\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True,\n",
        "                    dropout=0.5, name='encoder_lstm')\n",
        "_, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "\n",
        "# build the encoder network model\n",
        "encoder_model = Model(inputs=encoder_inputs,\n",
        "                      outputs=[state_h, state_c],\n",
        "                      name='encoder')"
      ],
      "metadata": {
        "id": "GvuaIHBHXiO8"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4vyzu9NYMzQ",
        "outputId": "6be46341-df15-466e-a9c2-1656486e8385"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_inputs (InputLayer  [(None, None, 28)]        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " encoder_lstm (LSTM)         [(None, 256),             291840    \n",
            "                              (None, 256),                       \n",
            "                              (None, 256)]                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 291840 (1.11 MB)\n",
            "Trainable params: 291840 (1.11 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### a2) Decoder network"
      ],
      "metadata": {
        "id": "UVlBaDidYSQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "# inputs of the decoder network\n",
        "decoder_input_h = Input(shape=(latent_dim,), name='decoder_input_h')\n",
        "decoder_input_c = Input(shape=(latent_dim,), name='decoder_input_c')\n",
        "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
        "\n",
        "# set the LSTM layer\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True,\n",
        "                    return_state=True, dropout=0.5, name='decoder_lstm')\n",
        "decoder_lstm_outputs, state_h, state_c = decoder_lstm(decoder_input_x,\n",
        "                                                      initial_state=[decoder_input_h, decoder_input_c])\n",
        "\n",
        "# set the dense layer\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
        "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
        "\n",
        "# build the decoder network model\n",
        "decoder_model = Model(inputs=[decoder_input_x, decoder_input_h, decoder_input_c],\n",
        "                      outputs=[decoder_outputs, state_h, state_c],\n",
        "                      name='decoder')"
      ],
      "metadata": {
        "id": "b2k3cvQJYQH6"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTwma9I_YV3B",
        "outputId": "300c72d4-c506-4828-eed1-c494332a68c6"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " decoder_input_x (InputLaye  [(None, None, 47)]           0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " decoder_input_h (InputLaye  [(None, 256)]                0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " decoder_input_c (InputLaye  [(None, 256)]                0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " decoder_lstm (LSTM)         [(None, None, 256),          311296    ['decoder_input_x[0][0]',     \n",
            "                              (None, 256),                           'decoder_input_h[0][0]',     \n",
            "                              (None, 256)]                           'decoder_input_c[0][0]']     \n",
            "                                                                                                  \n",
            " decoder_dense (Dense)       (None, None, 47)             12079     ['decoder_lstm[0][0]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 323375 (1.23 MB)\n",
            "Trainable params: 323375 (1.23 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9J0-HOKUfxR"
      },
      "source": [
        "#### a3) Connect the encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "2JX1Ra_wUfxR"
      },
      "outputs": [],
      "source": [
        "# input layers\n",
        "encoder_input_x = Input(shape=(None, num_encoder_tokens), name='encoder_input_x')\n",
        "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
        "\n",
        "# connect encoder to decoder\n",
        "encoder_final_states = encoder_model([encoder_input_x])\n",
        "decoder_lstm_output, _, _ = decoder_lstm(decoder_input_x, initial_state=encoder_final_states)\n",
        "decoder_pred = decoder_dense(decoder_lstm_output)\n",
        "\n",
        "model = Model(inputs=[encoder_input_x, decoder_input_x],\n",
        "              outputs=decoder_pred,\n",
        "              name='model_training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "cWLSBhN5UfxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1183229c-2e9c-4cf9-f572-3cd7a79fa337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.float32, name=None), name='decoder_lstm/PartitionedCall:2', description=\"created by layer 'decoder_lstm'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.float32, name='decoder_input_h'), name='decoder_input_h', description=\"created by layer 'decoder_input_h'\")\n"
          ]
        }
      ],
      "source": [
        "print(state_h)\n",
        "print(decoder_input_h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "LwOIVmInUfxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3b41ecc-8125-4db9-8e67-71c2cf29787b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_training\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_input_x (InputLaye  [(None, None, 28)]           0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " decoder_input_x (InputLaye  [(None, None, 47)]           0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " encoder (Functional)        [(None, 256),                291840    ['encoder_input_x[0][0]']     \n",
            "                              (None, 256)]                                                        \n",
            "                                                                                                  \n",
            " decoder_lstm (LSTM)         [(None, None, 256),          311296    ['decoder_input_x[0][0]',     \n",
            "                              (None, 256),                           'encoder[0][0]',             \n",
            "                              (None, 256)]                           'encoder[0][1]']             \n",
            "                                                                                                  \n",
            " decoder_dense (Dense)       (None, None, 47)             12079     ['decoder_lstm[1][0]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 615215 (2.35 MB)\n",
            "Trainable params: 615215 (2.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7arpzhx0UfxR"
      },
      "source": [
        "#### a4) Fit the model on the bilingual dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "bIMkVZeKUfxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b1544ab-58f6-4419-db7d-85331a45f0f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of encoder_input_data(28000, 20, 28)\n",
            "shape of decoder_input_data(28000, 44, 47)\n",
            "shape of decoder_target_data(28000, 44, 47)\n"
          ]
        }
      ],
      "source": [
        "print('shape of encoder_input_data' + str(encoder_input_data.shape))\n",
        "print('shape of decoder_input_data' + str(decoder_input_data.shape))\n",
        "print('shape of decoder_target_data' + str(decoder_target_data.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "zCCUDABEUfxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b420e86c-69c1-45ae-bd57-43070fb41cb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "350/350 [==============================] - 6s 9ms/step - loss: 1.0476 - val_loss: 0.8657\n",
            "Epoch 2/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.8833 - val_loss: 0.7966\n",
            "Epoch 3/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.8568 - val_loss: 0.7751\n",
            "Epoch 4/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.8450 - val_loss: 0.7568\n",
            "Epoch 5/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.8350 - val_loss: 0.7429\n",
            "Epoch 6/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.8267 - val_loss: 0.7323\n",
            "Epoch 7/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.8193 - val_loss: 0.7187\n",
            "Epoch 8/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.8138 - val_loss: 0.7121\n",
            "Epoch 9/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.8074 - val_loss: 0.7084\n",
            "Epoch 10/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.8031 - val_loss: 0.6985\n",
            "Epoch 11/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7990 - val_loss: 0.6929\n",
            "Epoch 12/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7961 - val_loss: 0.6843\n",
            "Epoch 13/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7924 - val_loss: 0.6803\n",
            "Epoch 14/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7885 - val_loss: 0.6751\n",
            "Epoch 15/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7858 - val_loss: 0.6719\n",
            "Epoch 16/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7840 - val_loss: 0.6724\n",
            "Epoch 17/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7805 - val_loss: 0.6692\n",
            "Epoch 18/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7792 - val_loss: 0.6611\n",
            "Epoch 19/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7759 - val_loss: 0.6617\n",
            "Epoch 20/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7736 - val_loss: 0.6565\n",
            "Epoch 21/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7723 - val_loss: 0.6494\n",
            "Epoch 22/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7712 - val_loss: 0.6501\n",
            "Epoch 23/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7689 - val_loss: 0.6465\n",
            "Epoch 24/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7667 - val_loss: 0.6413\n",
            "Epoch 25/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7650 - val_loss: 0.6438\n",
            "Epoch 26/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7637 - val_loss: 0.6401\n",
            "Epoch 27/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7619 - val_loss: 0.6398\n",
            "Epoch 28/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7614 - val_loss: 0.6336\n",
            "Epoch 29/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7599 - val_loss: 0.6323\n",
            "Epoch 30/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7590 - val_loss: 0.6309\n",
            "Epoch 31/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7575 - val_loss: 0.6312\n",
            "Epoch 32/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7559 - val_loss: 0.6278\n",
            "Epoch 33/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7544 - val_loss: 0.6262\n",
            "Epoch 34/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7529 - val_loss: 0.6255\n",
            "Epoch 35/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7535 - val_loss: 0.6233\n",
            "Epoch 36/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7520 - val_loss: 0.6209\n",
            "Epoch 37/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7507 - val_loss: 0.6196\n",
            "Epoch 38/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7496 - val_loss: 0.6204\n",
            "Epoch 39/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7483 - val_loss: 0.6184\n",
            "Epoch 40/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7477 - val_loss: 0.6167\n",
            "Epoch 41/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7457 - val_loss: 0.6150\n",
            "Epoch 42/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7454 - val_loss: 0.6145\n",
            "Epoch 43/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7444 - val_loss: 0.6123\n",
            "Epoch 44/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7443 - val_loss: 0.6100\n",
            "Epoch 45/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7436 - val_loss: 0.6085\n",
            "Epoch 46/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7429 - val_loss: 0.6073\n",
            "Epoch 47/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7434 - val_loss: 0.6086\n",
            "Epoch 48/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7408 - val_loss: 0.6059\n",
            "Epoch 49/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7407 - val_loss: 0.6042\n",
            "Epoch 50/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7387 - val_loss: 0.6039\n",
            "Epoch 51/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7393 - val_loss: 0.6023\n",
            "Epoch 52/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7386 - val_loss: 0.6023\n",
            "Epoch 53/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7368 - val_loss: 0.6009\n",
            "Epoch 54/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7368 - val_loss: 0.6007\n",
            "Epoch 55/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7353 - val_loss: 0.5998\n",
            "Epoch 56/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7367 - val_loss: 0.5991\n",
            "Epoch 57/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7343 - val_loss: 0.5973\n",
            "Epoch 58/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7342 - val_loss: 0.5980\n",
            "Epoch 59/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7334 - val_loss: 0.5939\n",
            "Epoch 60/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7324 - val_loss: 0.5921\n",
            "Epoch 61/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7318 - val_loss: 0.5926\n",
            "Epoch 62/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7307 - val_loss: 0.5925\n",
            "Epoch 63/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7320 - val_loss: 0.5952\n",
            "Epoch 64/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7306 - val_loss: 0.5922\n",
            "Epoch 65/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7312 - val_loss: 0.5895\n",
            "Epoch 66/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7297 - val_loss: 0.5878\n",
            "Epoch 67/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7281 - val_loss: 0.5891\n",
            "Epoch 68/100\n",
            "350/350 [==============================] - 2s 6ms/step - loss: 0.7282 - val_loss: 0.5880\n",
            "Epoch 69/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7267 - val_loss: 0.5876\n",
            "Epoch 70/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7269 - val_loss: 0.5873\n",
            "Epoch 71/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7259 - val_loss: 0.5872\n",
            "Epoch 72/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7262 - val_loss: 0.5865\n",
            "Epoch 73/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7243 - val_loss: 0.5842\n",
            "Epoch 74/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7241 - val_loss: 0.5839\n",
            "Epoch 75/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7247 - val_loss: 0.5846\n",
            "Epoch 76/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7234 - val_loss: 0.5824\n",
            "Epoch 77/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7230 - val_loss: 0.5822\n",
            "Epoch 78/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7235 - val_loss: 0.5809\n",
            "Epoch 79/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7225 - val_loss: 0.5811\n",
            "Epoch 80/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7206 - val_loss: 0.5820\n",
            "Epoch 81/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7230 - val_loss: 0.5804\n",
            "Epoch 82/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7198 - val_loss: 0.5794\n",
            "Epoch 83/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7205 - val_loss: 0.5794\n",
            "Epoch 84/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7189 - val_loss: 0.5800\n",
            "Epoch 85/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7205 - val_loss: 0.5779\n",
            "Epoch 86/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7196 - val_loss: 0.5793\n",
            "Epoch 87/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7183 - val_loss: 0.5770\n",
            "Epoch 88/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7177 - val_loss: 0.5777\n",
            "Epoch 89/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7176 - val_loss: 0.5774\n",
            "Epoch 90/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7167 - val_loss: 0.5763\n",
            "Epoch 91/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7177 - val_loss: 0.5755\n",
            "Epoch 92/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7162 - val_loss: 0.5755\n",
            "Epoch 93/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7168 - val_loss: 0.5747\n",
            "Epoch 94/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7157 - val_loss: 0.5737\n",
            "Epoch 95/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7155 - val_loss: 0.5739\n",
            "Epoch 96/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7158 - val_loss: 0.5736\n",
            "Epoch 97/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7159 - val_loss: 0.5721\n",
            "Epoch 98/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7129 - val_loss: 0.5722\n",
            "Epoch 99/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7134 - val_loss: 0.5721\n",
            "Epoch 100/100\n",
            "350/350 [==============================] - 2s 7ms/step - loss: 0.7139 - val_loss: 0.5724\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x786ce63f6a70>"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ],
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "\n",
        "model.fit([encoder_input_data, decoder_input_data],  # training data\n",
        "          decoder_target_data,                       # labels (left shift of the target sequences)\n",
        "          batch_size=64, epochs=100, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('seq2seq.keras')"
      ],
      "metadata": {
        "id": "rlUKLZ8k881t"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyztNgb6UfxV"
      },
      "source": [
        "#### a5) Make predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "O_7pbNrKUfxV"
      },
      "outputs": [],
      "source": [
        "# Reverse-lookup token index to decode sequences back to something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "uDBO5lnHUfxV"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=3)\n",
        "        # greedy selection\n",
        "        # sampled_token_index = numpy.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        # multinomial sampling with temperature\n",
        "        temperature=0.2\n",
        "        pred = output_tokens[0, -1, :] ** (1.0 / temperature)\n",
        "        temp_pred = pred / np.sum(pred)\n",
        "        sampled_token_index = np.random.choice(len(output_tokens[0, -1, :]), p=temp_pred)\n",
        "\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index+1]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        states_value = [h, c]\n",
        "        decoded_sentence = decoded_sentence[::-1]\n",
        "        #decoded_sentence = decoded_sentence.strip()\n",
        "    return decoded_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "MzHHw2yfUfxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c441b4d0-e25e-4b84-9a82-f56a6c9c45a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 311ms/step\n",
            "-\n",
            "English:        enjoy the game\n",
            "Your target language (true):  אתה יכול לטלפן לו\n",
            "Your target language (pred):  אחם\t\tבלהיויה\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "-\n",
            "English:        ill do anything\n",
            "Your target language (true):  אל תחששי לי\n",
            "Your target language (pred):  ןע\tו\tניטםותריהככיעמינ\n"
          ]
        }
      ],
      "source": [
        "for seq_index in range(5100, 5102):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    decoded_sentence = decoded_sentence.strip()\n",
        "    print('-')\n",
        "    print('English:       ', test_input_texts[seq_index])\n",
        "    print('Your target language (true): ', target_texts[seq_index][1:-1])\n",
        "    print('Your target language (pred): ', decoded_sentence[0:-1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N67pag4gUfxV"
      },
      "source": [
        "#### a6) Translate an English sentence to the target language"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = 'I love you'\n",
        "\n",
        "input_tokens = [char for char in input_sentence]\n",
        "\n",
        "input_sequence = text2sequences_heb(max_encoder_seq_length, [input_tokens], lang=\"Eng\")[0]\n",
        "\n",
        "input_x = onehot_encode(input_sequence, max_encoder_seq_length, num_encoder_tokens)\n",
        "\n",
        "translated_sentence = decode_sequence(input_x)\n",
        "translated_sentence = translated_sentence.strip()\n",
        "\n",
        "print('source sentence is: ' + input_sentence)\n",
        "print('translated sentence is: ' + translated_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhfvaFuIKVXf",
        "outputId": "fde0bec5-c427-43a5-e19e-99daba848a52"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 17ms/step\n",
            "source sentence is: I love you\n",
            "translated sentence is: קםמוננז\tת\tהילוי\tהיתא\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS0KfxSjUfxV"
      },
      "source": [
        "#### a7) Evaluate the translation using BLEU score (5 samples)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "individual_bleu_scores = []\n",
        "\n",
        "for seq_index in range(5000, 5005):\n",
        "\n",
        "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    decoded_sentence = decoded_sentence.strip()\n",
        "    actual_sentence = test_target_texts[seq_index][1:-1]\n",
        "    print(\"Decoded sentence: \", decoded_sentence)\n",
        "    print(\"Actual sentence: \", actual_sentence)\n",
        "    candidate = decoded_sentence.split()\n",
        "    references = actual_sentence.split()\n",
        "    # Calculate BLEU score for this sentence\n",
        "    bleu_score = sentence_bleu(references, candidate, weights=(1, 0, 0, 0))\n",
        "    print('Cumulative 1-gram: %f' % sentence_bleu(references, candidate, weights=(1, 0, 0, 0)))\n",
        "    print('Cumulative 2-gram: %f' % sentence_bleu(references, candidate, weights=(0.5, 0.5, 0, 0)))\n",
        "    print('Cumulative 3-gram: %f' % sentence_bleu(references, candidate, weights=(0.33, 0.33, 0.33, 0)))\n",
        "    print('Cumulative 4-gram: %f' % sentence_bleu(references, candidate, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "    individual_bleu_scores.append(bleu_score)\n",
        "\n",
        "# Calculate the average BLEU score\n",
        "average_bleu_score = sum(individual_bleu_scores) / len(individual_bleu_scores)\n",
        "\n",
        "print(\"Average BLEU Score on the Test Set:\", average_bleu_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ta6NDn3gigmv",
        "outputId": "76164e00-bec7-4764-cc8d-37e769c3d226"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "Decoded sentence:  יתלענמעיטםותריהככהיכהילדא\n",
            "Actual sentence:  תום תמיד משקר\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "Decoded sentence:  קז\tיחםותריהמנלוא\n",
            "Actual sentence:  אתה יכול להביא את זה\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "Decoded sentence:  אלתלשםת\tויהמיהו\n",
            "Actual sentence:  אני אוהב אותם\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Decoded sentence:  מז\tיחםותריהמנלוא\n",
            "Actual sentence:  לעולם אינני דואג\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Decoded sentence:  אהם\t\tת\tהיויתמ\n",
            "Actual sentence:  אנו לא מוכנות\n",
            "Cumulative 1-gram: 0.333333\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "Average BLEU Score on the Test Set: 0.06666666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b) Bi-LSTM Seq2Seq model\n",
        "\n",
        "Combinations tried:\n",
        "\n",
        "- Latent dimensions - 128/256/512\n",
        "- training epochs - 10, 25, 50\n",
        "- dropout = 0.2, 0.5\n",
        "- activation function - RMS, Adam\n",
        "- sampling - greedy, multinomial (different temperatures)"
      ],
      "metadata": {
        "id": "XwiZa4O6wtme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### b1) Encoder network"
      ],
      "metadata": {
        "id": "vmJk-hWFwtme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Bidirectional, Concatenate\n",
        "from keras.layers import Input, LSTM\n",
        "from keras.models import Model\n",
        "\n",
        "latent_dim = 128\n",
        "\n",
        "# inputs of the encoder network\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens),\n",
        "                       name='encoder_inputs')\n",
        "\n",
        "encoder_bilstm = Bidirectional(LSTM(latent_dim, return_state=True,\n",
        "                                  dropout=0.5, name='encoder_lstm'))\n",
        "_, forward_h, forward_c, backward_h, backward_c = encoder_bilstm(encoder_inputs)\n",
        "\n",
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "\n",
        "encoder_model = Model(inputs=encoder_inputs,\n",
        "                      outputs=[state_h, state_c],\n",
        "                      name='encoder')"
      ],
      "metadata": {
        "id": "DS4XyjxtxKaB"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e790875-0bf6-4b4d-b459-a82bd0dbe370",
        "id": "4PLqv_Q_wtmf"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer  [(None, None, 28)]           0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirecti  [(None, 256),                160768    ['encoder_inputs[0][0]']      \n",
            " onal)                        (None, 128),                                                        \n",
            "                              (None, 128),                                                        \n",
            "                              (None, 128),                                                        \n",
            "                              (None, 128)]                                                        \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 256)                  0         ['bidirectional_1[0][1]',     \n",
            " )                                                                   'bidirectional_1[0][3]']     \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate  (None, 256)                  0         ['bidirectional_1[0][2]',     \n",
            " )                                                                   'bidirectional_1[0][4]']     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 160768 (628.00 KB)\n",
            "Trainable params: 160768 (628.00 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### b2) Decoder network"
      ],
      "metadata": {
        "id": "ktbEIKcpwtmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "# inputs of the decoder network\n",
        "decoder_input_h = Input(shape=(latent_dim*2,), name='decoder_input_h')\n",
        "decoder_input_c = Input(shape=(latent_dim*2,), name='decoder_input_c')\n",
        "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
        "\n",
        "# set the LSTM layer\n",
        "decoder_lstm = LSTM(latent_dim*2, return_sequences=True,\n",
        "                    return_state=True, dropout=0.5, name='decoder_lstm')\n",
        "decoder_lstm_outputs, state_h, state_c = decoder_lstm(decoder_input_x,\n",
        "                                                      initial_state=[decoder_input_h, decoder_input_c])\n",
        "\n",
        "# set the dense layer\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
        "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
        "\n",
        "# build the decoder network model\n",
        "decoder_model = Model(inputs=[decoder_input_x, decoder_input_h, decoder_input_c],\n",
        "                      outputs=[decoder_outputs, state_h, state_c],\n",
        "                      name='decoder')"
      ],
      "metadata": {
        "id": "_SOgk2Sbwtmf"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0a5c3aa-b5f9-4f3c-9f60-299b3e4abeba",
        "id": "1wHoQYhlwtmf"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " decoder_input_x (InputLaye  [(None, None, 47)]           0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " decoder_input_h (InputLaye  [(None, 256)]                0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " decoder_input_c (InputLaye  [(None, 256)]                0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " decoder_lstm (LSTM)         [(None, None, 256),          311296    ['decoder_input_x[0][0]',     \n",
            "                              (None, 256),                           'decoder_input_h[0][0]',     \n",
            "                              (None, 256)]                           'decoder_input_c[0][0]']     \n",
            "                                                                                                  \n",
            " decoder_dense (Dense)       (None, None, 47)             12079     ['decoder_lstm[0][0]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 323375 (1.23 MB)\n",
            "Trainable params: 323375 (1.23 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_h6cmzpwtmf"
      },
      "source": [
        "#### b3) Connect the encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "1rNtuvL7wtmg"
      },
      "outputs": [],
      "source": [
        "# input layers\n",
        "encoder_input_x = Input(shape=(None, num_encoder_tokens), name='encoder_input_x')\n",
        "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
        "\n",
        "# connect encoder to decoder\n",
        "encoder_final_states = encoder_model([encoder_input_x])\n",
        "decoder_lstm_output, _, _ = decoder_lstm(decoder_input_x, initial_state=encoder_final_states)\n",
        "decoder_pred = decoder_dense(decoder_lstm_output)\n",
        "\n",
        "model = Model(inputs=[encoder_input_x, decoder_input_x],\n",
        "              outputs=decoder_pred,\n",
        "              name='model_training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8858ecc-765c-4302-87d8-1ecda39557e5",
        "id": "9X7A-n9Awtmg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.float32, name=None), name='decoder_lstm/PartitionedCall:2', description=\"created by layer 'decoder_lstm'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.float32, name='decoder_input_h'), name='decoder_input_h', description=\"created by layer 'decoder_input_h'\")\n"
          ]
        }
      ],
      "source": [
        "print(state_h)\n",
        "print(decoder_input_h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4cb9ffb-52a0-4e54-c4d9-acb81fd74589",
        "id": "L1u-ZlGYwtmg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_training\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_input_x (InputLaye  [(None, None, 28)]           0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " decoder_input_x (InputLaye  [(None, None, 47)]           0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " encoder (Functional)        [(None, 256),                160768    ['encoder_input_x[0][0]']     \n",
            "                              (None, 256)]                                                        \n",
            "                                                                                                  \n",
            " decoder_lstm (LSTM)         [(None, None, 256),          311296    ['decoder_input_x[0][0]',     \n",
            "                              (None, 256),                           'encoder[0][0]',             \n",
            "                              (None, 256)]                           'encoder[0][1]']             \n",
            "                                                                                                  \n",
            " decoder_dense (Dense)       (None, None, 47)             12079     ['decoder_lstm[1][0]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 484143 (1.85 MB)\n",
            "Trainable params: 484143 (1.85 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b98ex1HHwtmg"
      },
      "source": [
        "#### b4) Fit the model on the bilingual dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dcce195-4222-42f6-b189-6f2ffe2225a3",
        "id": "pOiMBXKdwtmg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of encoder_input_data(28000, 20, 28)\n",
            "shape of decoder_input_data(28000, 44, 47)\n",
            "shape of decoder_target_data(28000, 44, 47)\n"
          ]
        }
      ],
      "source": [
        "print('shape of encoder_input_data' + str(encoder_input_data.shape))\n",
        "print('shape of decoder_input_data' + str(decoder_input_data.shape))\n",
        "print('shape of decoder_target_data' + str(decoder_target_data.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "hWST5TnKwtmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04f11bd7-022e-4026-c15c-05f2e6b494fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "350/350 [==============================] - 8s 12ms/step - loss: 1.0585 - val_loss: 0.8295\n",
            "Epoch 2/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.8633 - val_loss: 0.7750\n",
            "Epoch 3/100\n",
            "350/350 [==============================] - 3s 9ms/step - loss: 0.8403 - val_loss: 0.7509\n",
            "Epoch 4/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.8270 - val_loss: 0.7349\n",
            "Epoch 5/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.8162 - val_loss: 0.7178\n",
            "Epoch 6/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.8066 - val_loss: 0.7043\n",
            "Epoch 7/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.8006 - val_loss: 0.6944\n",
            "Epoch 8/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7935 - val_loss: 0.6841\n",
            "Epoch 9/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7877 - val_loss: 0.6748\n",
            "Epoch 10/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7847 - val_loss: 0.6661\n",
            "Epoch 11/100\n",
            "350/350 [==============================] - 3s 9ms/step - loss: 0.7773 - val_loss: 0.6599\n",
            "Epoch 12/100\n",
            "350/350 [==============================] - 3s 9ms/step - loss: 0.7742 - val_loss: 0.6545\n",
            "Epoch 13/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7698 - val_loss: 0.6465\n",
            "Epoch 14/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7658 - val_loss: 0.6410\n",
            "Epoch 15/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7625 - val_loss: 0.6362\n",
            "Epoch 16/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7583 - val_loss: 0.6289\n",
            "Epoch 17/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7566 - val_loss: 0.6272\n",
            "Epoch 18/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7545 - val_loss: 0.6233\n",
            "Epoch 19/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7503 - val_loss: 0.6185\n",
            "Epoch 20/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7483 - val_loss: 0.6169\n",
            "Epoch 21/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7465 - val_loss: 0.6122\n",
            "Epoch 22/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7429 - val_loss: 0.6090\n",
            "Epoch 23/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7411 - val_loss: 0.6056\n",
            "Epoch 24/100\n",
            "350/350 [==============================] - 3s 9ms/step - loss: 0.7402 - val_loss: 0.6023\n",
            "Epoch 25/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7372 - val_loss: 0.5997\n",
            "Epoch 26/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7355 - val_loss: 0.5962\n",
            "Epoch 27/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7344 - val_loss: 0.5953\n",
            "Epoch 28/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7327 - val_loss: 0.5924\n",
            "Epoch 29/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7306 - val_loss: 0.5902\n",
            "Epoch 30/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7308 - val_loss: 0.5896\n",
            "Epoch 31/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7270 - val_loss: 0.5863\n",
            "Epoch 32/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7276 - val_loss: 0.5838\n",
            "Epoch 33/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7249 - val_loss: 0.5844\n",
            "Epoch 34/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7235 - val_loss: 0.5823\n",
            "Epoch 35/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7226 - val_loss: 0.5805\n",
            "Epoch 36/100\n",
            "350/350 [==============================] - 3s 9ms/step - loss: 0.7211 - val_loss: 0.5803\n",
            "Epoch 37/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7194 - val_loss: 0.5789\n",
            "Epoch 38/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7188 - val_loss: 0.5761\n",
            "Epoch 39/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7189 - val_loss: 0.5744\n",
            "Epoch 40/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7182 - val_loss: 0.5752\n",
            "Epoch 41/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7159 - val_loss: 0.5730\n",
            "Epoch 42/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7151 - val_loss: 0.5722\n",
            "Epoch 43/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7143 - val_loss: 0.5699\n",
            "Epoch 44/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7134 - val_loss: 0.5691\n",
            "Epoch 45/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7128 - val_loss: 0.5693\n",
            "Epoch 46/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7123 - val_loss: 0.5682\n",
            "Epoch 47/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7117 - val_loss: 0.5670\n",
            "Epoch 48/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7109 - val_loss: 0.5669\n",
            "Epoch 49/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7110 - val_loss: 0.5643\n",
            "Epoch 50/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7083 - val_loss: 0.5648\n",
            "Epoch 51/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7075 - val_loss: 0.5630\n",
            "Epoch 52/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7076 - val_loss: 0.5647\n",
            "Epoch 53/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7064 - val_loss: 0.5617\n",
            "Epoch 54/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7052 - val_loss: 0.5617\n",
            "Epoch 55/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7054 - val_loss: 0.5603\n",
            "Epoch 56/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7051 - val_loss: 0.5614\n",
            "Epoch 57/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7035 - val_loss: 0.5591\n",
            "Epoch 58/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7041 - val_loss: 0.5590\n",
            "Epoch 59/100\n",
            "350/350 [==============================] - 4s 10ms/step - loss: 0.7023 - val_loss: 0.5575\n",
            "Epoch 60/100\n",
            "350/350 [==============================] - 3s 9ms/step - loss: 0.7029 - val_loss: 0.5575\n",
            "Epoch 61/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7018 - val_loss: 0.5561\n",
            "Epoch 62/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7008 - val_loss: 0.5587\n",
            "Epoch 63/100\n",
            "350/350 [==============================] - 3s 9ms/step - loss: 0.7022 - val_loss: 0.5571\n",
            "Epoch 64/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6996 - val_loss: 0.5557\n",
            "Epoch 65/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.7008 - val_loss: 0.5566\n",
            "Epoch 66/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6983 - val_loss: 0.5548\n",
            "Epoch 67/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6966 - val_loss: 0.5547\n",
            "Epoch 68/100\n",
            "350/350 [==============================] - 3s 9ms/step - loss: 0.6982 - val_loss: 0.5543\n",
            "Epoch 69/100\n",
            "350/350 [==============================] - 3s 9ms/step - loss: 0.6968 - val_loss: 0.5520\n",
            "Epoch 70/100\n",
            "350/350 [==============================] - 3s 9ms/step - loss: 0.6977 - val_loss: 0.5548\n",
            "Epoch 71/100\n",
            "350/350 [==============================] - 3s 9ms/step - loss: 0.6953 - val_loss: 0.5541\n",
            "Epoch 72/100\n",
            "350/350 [==============================] - 3s 9ms/step - loss: 0.6970 - val_loss: 0.5527\n",
            "Epoch 73/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6947 - val_loss: 0.5528\n",
            "Epoch 74/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6963 - val_loss: 0.5525\n",
            "Epoch 75/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6923 - val_loss: 0.5550\n",
            "Epoch 76/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6948 - val_loss: 0.5515\n",
            "Epoch 77/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6954 - val_loss: 0.5532\n",
            "Epoch 78/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6929 - val_loss: 0.5526\n",
            "Epoch 79/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6937 - val_loss: 0.5519\n",
            "Epoch 80/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6929 - val_loss: 0.5515\n",
            "Epoch 81/100\n",
            "350/350 [==============================] - 3s 9ms/step - loss: 0.6926 - val_loss: 0.5506\n",
            "Epoch 82/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6922 - val_loss: 0.5514\n",
            "Epoch 83/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6925 - val_loss: 0.5514\n",
            "Epoch 84/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6915 - val_loss: 0.5515\n",
            "Epoch 85/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6913 - val_loss: 0.5500\n",
            "Epoch 86/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6920 - val_loss: 0.5509\n",
            "Epoch 87/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6913 - val_loss: 0.5492\n",
            "Epoch 88/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6906 - val_loss: 0.5488\n",
            "Epoch 89/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6903 - val_loss: 0.5494\n",
            "Epoch 90/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6904 - val_loss: 0.5498\n",
            "Epoch 91/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6881 - val_loss: 0.5494\n",
            "Epoch 92/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6879 - val_loss: 0.5499\n",
            "Epoch 93/100\n",
            "350/350 [==============================] - 3s 9ms/step - loss: 0.6901 - val_loss: 0.5489\n",
            "Epoch 94/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6895 - val_loss: 0.5497\n",
            "Epoch 95/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6886 - val_loss: 0.5479\n",
            "Epoch 96/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6873 - val_loss: 0.5482\n",
            "Epoch 97/100\n",
            "350/350 [==============================] - 3s 9ms/step - loss: 0.6876 - val_loss: 0.5481\n",
            "Epoch 98/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6869 - val_loss: 0.5481\n",
            "Epoch 99/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6863 - val_loss: 0.5472\n",
            "Epoch 100/100\n",
            "350/350 [==============================] - 3s 8ms/step - loss: 0.6859 - val_loss: 0.5477\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x786ce6d9ac80>"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "model.fit([encoder_input_data, decoder_input_data],  # training data\n",
        "          decoder_target_data,                       # labels (left shift of the target sequences)\n",
        "          batch_size=64, epochs=100, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('seq2se_bilstm.keras')"
      ],
      "metadata": {
        "id": "cL5OEnybwtmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsibbVGMwtmg"
      },
      "source": [
        "#### b5) Make predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "jOKs-hOiwtmg"
      },
      "outputs": [],
      "source": [
        "# Reverse-lookup token index to decode sequences back to something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "fFX-q-Wbwtmg"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=3)\n",
        "        # greedy selection\n",
        "        # sampled_token_index = numpy.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        # multinomial sampling with temperature\n",
        "        temperature=0.2\n",
        "        pred = output_tokens[0, -1, :] ** (1.0 / temperature)\n",
        "        temp_pred = pred / np.sum(pred)\n",
        "        sampled_token_index = np.random.choice(len(output_tokens[0, -1, :]), p=temp_pred)\n",
        "\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index+1]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        states_value = [h, c]\n",
        "        decoded_sentence = decoded_sentence[::-1]\n",
        "\n",
        "    return decoded_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in range(1200, 1202):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    decoded_sentence = decoded_sentence.strip()\n",
        "    print('-')\n",
        "    print('English:       ', test_input_texts[seq_index])\n",
        "    print('Your target language (true): ', target_texts[seq_index][1:-1])\n",
        "    print('Your target language (pred): ', decoded_sentence[0:-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvU4dt7oV6u7",
        "outputId": "cace7b43-b66c-46f9-9977-b17dab17633b"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 592ms/step\n",
            "-\n",
            "English:        tom ate\n",
            "Your target language (true):  אנו משוכנעים\n",
            "Your target language (pred):  אהשעטםותריהוים\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "-\n",
            "English:        tell the truth\n",
            "Your target language (true):  זה עוזר\n",
            "Your target language (pred):  נהם\t\tבלהיוישנ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOXIxdQKwtmh"
      },
      "source": [
        "#### b6) Translate an English sentence to the target language"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = 'I love you'\n",
        "\n",
        "input_tokens = [char for char in input_sentence]\n",
        "\n",
        "input_sequence = text2sequences_heb(max_encoder_seq_length, [input_tokens], lang=\"Eng\")[0]\n",
        "\n",
        "input_x = onehot_encode(input_sequence, max_encoder_seq_length, num_encoder_tokens)\n",
        "\n",
        "translated_sentence = decode_sequence(input_x)\n",
        "translated_sentence = translated_sentence.strip()\n",
        "\n",
        "print('source sentence is: ' + input_sentence)\n",
        "print('translated sentence is: ' + translated_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87b81fae-3443-451f-e19c-2759a460f414",
        "id": "xC1kvpQ-wtmh"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "source sentence is: I love you\n",
            "translated sentence is: אחם\t\tבלהיויהף\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrlAkxhpwtmh"
      },
      "source": [
        "#### b7) Evaluate the translation using BLEU score (for 5 samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "individual_bleu_scores = []\n",
        "\n",
        "for seq_index in range(5000, 5005):\n",
        "\n",
        "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    decoded_sentence = decoded_sentence.strip()\n",
        "    actual_sentence = test_target_texts[seq_index][1:-1]\n",
        "    print(\"Decoded sentence: \", decoded_sentence)\n",
        "    print(\"Actual sentence: \", actual_sentence)\n",
        "    candidate = decoded_sentence.split()\n",
        "    references = actual_sentence.split()\n",
        "    # Calculate BLEU score for this sentence\n",
        "    bleu_score = sentence_bleu(references, candidate, weights=(1, 0, 0, 0))\n",
        "    print('Cumulative 1-gram: %f' % sentence_bleu(references, candidate, weights=(1, 0, 0, 0)))\n",
        "    print('Cumulative 2-gram: %f' % sentence_bleu(references, candidate, weights=(0.5, 0.5, 0, 0)))\n",
        "    print('Cumulative 3-gram: %f' % sentence_bleu(references, candidate, weights=(0.33, 0.33, 0.33, 0)))\n",
        "    print('Cumulative 4-gram: %f' % sentence_bleu(references, candidate, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "    individual_bleu_scores.append(bleu_score)\n",
        "\n",
        "# Calculate the average BLEU score\n",
        "average_bleu_score = sum(individual_bleu_scores) / len(individual_bleu_scores)\n",
        "\n",
        "print(\"Average BLEU Score on the Test Set:\", average_bleu_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74020846-eae6-4f44-dd82-713d298df449",
        "id": "r-TzwCGJwtmi"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n",
            "Decoded sentence:  אונלטטםותרילככיםב\n",
            "Actual sentence:  תום תמיד משקר\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Decoded sentence:  אוחיחנזחםותריהמינלכעה\n",
            "Actual sentence:  אתה יכול להביא את זה\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Decoded sentence:  אהם\t\tת\tהיוישז\n",
            "Actual sentence:  אני אוהב אותם\n",
            "Cumulative 1-gram: 0.333333\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Decoded sentence:  אשםעטםותרימויהמ\n",
            "Actual sentence:  לעולם אינני דואג\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Decoded sentence:  אהם\t\tת\tהיוישז\n",
            "Actual sentence:  אנו לא מוכנות\n",
            "Cumulative 1-gram: 0.333333\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "Average BLEU Score on the Test Set: 0.13333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c) Bi-LSTM Seq2Seq model - Hebrew pre-padding, more training epochs and higher dimensions in hidden state\n",
        "\n",
        "Combinations used:\n",
        "\n",
        "- Latent dimension - 512\n",
        "- training epochs - 75\n",
        "- dropout = 0.5\n",
        "- activation function - Adam\n",
        "- sampling - multinomial (temperature = 0.25)"
      ],
      "metadata": {
        "id": "W8F65qSkiPyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### c1) Encoder network"
      ],
      "metadata": {
        "id": "V5d7gJ8wiPyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Bidirectional, Concatenate\n",
        "from keras.layers import Input, LSTM\n",
        "from keras.models import Model\n",
        "\n",
        "latent_dim = 512\n",
        "\n",
        "# inputs of the encoder network\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens),\n",
        "                       name='encoder_inputs')\n",
        "\n",
        "encoder_bilstm = Bidirectional(LSTM(latent_dim, return_state=True,\n",
        "                                  dropout=0.5, name='encoder_lstm'))\n",
        "_, forward_h, forward_c, backward_h, backward_c = encoder_bilstm(encoder_inputs)\n",
        "\n",
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "\n",
        "encoder_model = Model(inputs=encoder_inputs,\n",
        "                      outputs=[state_h, state_c],\n",
        "                      name='encoder')"
      ],
      "metadata": {
        "id": "VONCxmlqiPyr"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5df4478d-7ed7-4601-8606-6db6cd217750",
        "id": "lKXX-5GTiPyr"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer  [(None, None, 28)]           0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bidirectional_2 (Bidirecti  [(None, 1024),               2215936   ['encoder_inputs[0][0]']      \n",
            " onal)                        (None, 512),                                                        \n",
            "                              (None, 512),                                                        \n",
            "                              (None, 512),                                                        \n",
            "                              (None, 512)]                                                        \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate  (None, 1024)                 0         ['bidirectional_2[0][1]',     \n",
            " )                                                                   'bidirectional_2[0][3]']     \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate  (None, 1024)                 0         ['bidirectional_2[0][2]',     \n",
            " )                                                                   'bidirectional_2[0][4]']     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2215936 (8.45 MB)\n",
            "Trainable params: 2215936 (8.45 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### c2) Decoder network"
      ],
      "metadata": {
        "id": "qvnCxv8uiPyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "# inputs of the decoder network\n",
        "decoder_input_h = Input(shape=(latent_dim*2,), name='decoder_input_h')\n",
        "decoder_input_c = Input(shape=(latent_dim*2,), name='decoder_input_c')\n",
        "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
        "\n",
        "# set the LSTM layer\n",
        "decoder_lstm = LSTM(latent_dim*2, return_sequences=True,\n",
        "                    return_state=True, dropout=0.5, name='decoder_lstm')\n",
        "decoder_lstm_outputs, state_h, state_c = decoder_lstm(decoder_input_x,\n",
        "                                                      initial_state=[decoder_input_h, decoder_input_c])\n",
        "\n",
        "# set the dense layer\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
        "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
        "\n",
        "# build the decoder network model\n",
        "decoder_model = Model(inputs=[decoder_input_x, decoder_input_h, decoder_input_c],\n",
        "                      outputs=[decoder_outputs, state_h, state_c],\n",
        "                      name='decoder')"
      ],
      "metadata": {
        "id": "sAusQVZGiPyr"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44f2c8b4-de47-4065-8210-b5668261800a",
        "id": "wb41WgrpiPyr"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " decoder_input_x (InputLaye  [(None, None, 47)]           0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " decoder_input_h (InputLaye  [(None, 1024)]               0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " decoder_input_c (InputLaye  [(None, 1024)]               0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " decoder_lstm (LSTM)         [(None, None, 1024),         4390912   ['decoder_input_x[0][0]',     \n",
            "                              (None, 1024),                          'decoder_input_h[0][0]',     \n",
            "                              (None, 1024)]                          'decoder_input_c[0][0]']     \n",
            "                                                                                                  \n",
            " decoder_dense (Dense)       (None, None, 47)             48175     ['decoder_lstm[0][0]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4439087 (16.93 MB)\n",
            "Trainable params: 4439087 (16.93 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEG4y00siPys"
      },
      "source": [
        "#### c3) Connect the encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "fnpXVQfziPys"
      },
      "outputs": [],
      "source": [
        "# input layers\n",
        "encoder_input_x = Input(shape=(None, num_encoder_tokens), name='encoder_input_x')\n",
        "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
        "\n",
        "# connect encoder to decoder\n",
        "encoder_final_states = encoder_model([encoder_input_x])\n",
        "decoder_lstm_output, _, _ = decoder_lstm(decoder_input_x, initial_state=encoder_final_states)\n",
        "decoder_pred = decoder_dense(decoder_lstm_output)\n",
        "\n",
        "model = Model(inputs=[encoder_input_x, decoder_input_x],\n",
        "              outputs=decoder_pred,\n",
        "              name='model_training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80684813-0582-4df0-d908-ce371d15b251",
        "id": "ydq_DRcsiPys"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 1024), dtype=tf.float32, name=None), name='decoder_lstm/PartitionedCall:2', description=\"created by layer 'decoder_lstm'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 1024), dtype=tf.float32, name='decoder_input_h'), name='decoder_input_h', description=\"created by layer 'decoder_input_h'\")\n"
          ]
        }
      ],
      "source": [
        "print(state_h)\n",
        "print(decoder_input_h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e300e35-ddab-4542-8e36-36cb3a3d70a9",
        "id": "EtXZfKmAiPys"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_training\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_input_x (InputLaye  [(None, None, 28)]           0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " decoder_input_x (InputLaye  [(None, None, 47)]           0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " encoder (Functional)        [(None, 1024),               2215936   ['encoder_input_x[0][0]']     \n",
            "                              (None, 1024)]                                                       \n",
            "                                                                                                  \n",
            " decoder_lstm (LSTM)         [(None, None, 1024),         4390912   ['decoder_input_x[0][0]',     \n",
            "                              (None, 1024),                          'encoder[0][0]',             \n",
            "                              (None, 1024)]                          'encoder[0][1]']             \n",
            "                                                                                                  \n",
            " decoder_dense (Dense)       (None, None, 47)             48175     ['decoder_lstm[1][0]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6655023 (25.39 MB)\n",
            "Trainable params: 6655023 (25.39 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSSc7RyYiPys"
      },
      "source": [
        "#### c4) Fit the model on the bilingual dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10f7837a-de60-4094-858f-4e0f2219ab46",
        "id": "sVvzQO-eiPys"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of encoder_input_data(28000, 20, 28)\n",
            "shape of decoder_input_data(28000, 44, 47)\n",
            "shape of decoder_target_data(28000, 44, 47)\n"
          ]
        }
      ],
      "source": [
        "print('shape of encoder_input_data' + str(encoder_input_data.shape))\n",
        "print('shape of decoder_input_data' + str(decoder_input_data.shape))\n",
        "print('shape of decoder_target_data' + str(decoder_target_data.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e41b1af-a385-4445-8f74-234c55803389",
        "id": "sm9F2mBGiPys"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "350/350 [==============================] - 12s 23ms/step - loss: 0.9820 - val_loss: 0.8000\n",
            "Epoch 2/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.8450 - val_loss: 0.7503\n",
            "Epoch 3/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.8199 - val_loss: 0.7178\n",
            "Epoch 4/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.8004 - val_loss: 0.6912\n",
            "Epoch 5/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.7855 - val_loss: 0.6662\n",
            "Epoch 6/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.7732 - val_loss: 0.6520\n",
            "Epoch 7/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.7657 - val_loss: 0.6387\n",
            "Epoch 8/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.7579 - val_loss: 0.6277\n",
            "Epoch 9/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.7501 - val_loss: 0.6221\n",
            "Epoch 10/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.7442 - val_loss: 0.6111\n",
            "Epoch 11/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.7387 - val_loss: 0.6029\n",
            "Epoch 12/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.7337 - val_loss: 0.5974\n",
            "Epoch 13/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.7270 - val_loss: 0.5903\n",
            "Epoch 14/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.7229 - val_loss: 0.5865\n",
            "Epoch 15/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.7194 - val_loss: 0.5828\n",
            "Epoch 16/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.7152 - val_loss: 0.5752\n",
            "Epoch 17/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.7096 - val_loss: 0.5705\n",
            "Epoch 18/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.7059 - val_loss: 0.5679\n",
            "Epoch 19/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.7042 - val_loss: 0.5657\n",
            "Epoch 20/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.7007 - val_loss: 0.5613\n",
            "Epoch 21/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6964 - val_loss: 0.5586\n",
            "Epoch 22/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6929 - val_loss: 0.5570\n",
            "Epoch 23/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6899 - val_loss: 0.5545\n",
            "Epoch 24/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6878 - val_loss: 0.5520\n",
            "Epoch 25/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6824 - val_loss: 0.5495\n",
            "Epoch 26/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6830 - val_loss: 0.5502\n",
            "Epoch 27/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6762 - val_loss: 0.5495\n",
            "Epoch 28/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6729 - val_loss: 0.5449\n",
            "Epoch 29/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6712 - val_loss: 0.5447\n",
            "Epoch 30/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6672 - val_loss: 0.5423\n",
            "Epoch 31/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6644 - val_loss: 0.5434\n",
            "Epoch 32/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6628 - val_loss: 0.5421\n",
            "Epoch 33/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6625 - val_loss: 0.5422\n",
            "Epoch 34/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6591 - val_loss: 0.5413\n",
            "Epoch 35/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6568 - val_loss: 0.5411\n",
            "Epoch 36/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6554 - val_loss: 0.5398\n",
            "Epoch 37/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6542 - val_loss: 0.5395\n",
            "Epoch 38/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6512 - val_loss: 0.5392\n",
            "Epoch 39/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6505 - val_loss: 0.5388\n",
            "Epoch 40/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6457 - val_loss: 0.5383\n",
            "Epoch 41/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6463 - val_loss: 0.5396\n",
            "Epoch 42/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6421 - val_loss: 0.5391\n",
            "Epoch 43/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6412 - val_loss: 0.5396\n",
            "Epoch 44/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6404 - val_loss: 0.5397\n",
            "Epoch 45/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6364 - val_loss: 0.5382\n",
            "Epoch 46/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6345 - val_loss: 0.5404\n",
            "Epoch 47/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6357 - val_loss: 0.5389\n",
            "Epoch 48/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6328 - val_loss: 0.5414\n",
            "Epoch 49/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6301 - val_loss: 0.5414\n",
            "Epoch 50/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6295 - val_loss: 0.5422\n",
            "Epoch 51/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6270 - val_loss: 0.5424\n",
            "Epoch 52/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6242 - val_loss: 0.5423\n",
            "Epoch 53/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6224 - val_loss: 0.5417\n",
            "Epoch 54/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.6218 - val_loss: 0.5432\n",
            "Epoch 55/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6206 - val_loss: 0.5420\n",
            "Epoch 56/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6201 - val_loss: 0.5417\n",
            "Epoch 57/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6155 - val_loss: 0.5419\n",
            "Epoch 58/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6121 - val_loss: 0.5417\n",
            "Epoch 59/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6137 - val_loss: 0.5434\n",
            "Epoch 60/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6137 - val_loss: 0.5465\n",
            "Epoch 61/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6098 - val_loss: 0.5468\n",
            "Epoch 62/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6074 - val_loss: 0.5427\n",
            "Epoch 63/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6077 - val_loss: 0.5477\n",
            "Epoch 64/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6072 - val_loss: 0.5480\n",
            "Epoch 65/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6044 - val_loss: 0.5508\n",
            "Epoch 66/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6058 - val_loss: 0.5468\n",
            "Epoch 67/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6046 - val_loss: 0.5478\n",
            "Epoch 68/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.6005 - val_loss: 0.5485\n",
            "Epoch 69/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5981 - val_loss: 0.5452\n",
            "Epoch 70/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5973 - val_loss: 0.5468\n",
            "Epoch 71/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5981 - val_loss: 0.5483\n",
            "Epoch 72/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5958 - val_loss: 0.5518\n",
            "Epoch 73/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5940 - val_loss: 0.5498\n",
            "Epoch 74/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5919 - val_loss: 0.5511\n",
            "Epoch 75/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5923 - val_loss: 0.5545\n",
            "Epoch 76/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5910 - val_loss: 0.5573\n",
            "Epoch 77/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5902 - val_loss: 0.5546\n",
            "Epoch 78/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5903 - val_loss: 0.5513\n",
            "Epoch 79/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5866 - val_loss: 0.5538\n",
            "Epoch 80/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5853 - val_loss: 0.5563\n",
            "Epoch 81/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5844 - val_loss: 0.5576\n",
            "Epoch 82/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5839 - val_loss: 0.5580\n",
            "Epoch 83/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5818 - val_loss: 0.5586\n",
            "Epoch 84/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5792 - val_loss: 0.5575\n",
            "Epoch 85/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5811 - val_loss: 0.5577\n",
            "Epoch 86/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5773 - val_loss: 0.5590\n",
            "Epoch 87/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5749 - val_loss: 0.5584\n",
            "Epoch 88/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5755 - val_loss: 0.5583\n",
            "Epoch 89/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5747 - val_loss: 0.5613\n",
            "Epoch 90/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5726 - val_loss: 0.5621\n",
            "Epoch 91/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5739 - val_loss: 0.5632\n",
            "Epoch 92/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5711 - val_loss: 0.5645\n",
            "Epoch 93/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5693 - val_loss: 0.5643\n",
            "Epoch 94/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5685 - val_loss: 0.5664\n",
            "Epoch 95/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5676 - val_loss: 0.5666\n",
            "Epoch 96/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5645 - val_loss: 0.5660\n",
            "Epoch 97/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5637 - val_loss: 0.5673\n",
            "Epoch 98/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5657 - val_loss: 0.5663\n",
            "Epoch 99/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5636 - val_loss: 0.5693\n",
            "Epoch 100/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5631 - val_loss: 0.5697\n",
            "Epoch 101/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5628 - val_loss: 0.5718\n",
            "Epoch 102/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5606 - val_loss: 0.5727\n",
            "Epoch 103/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5579 - val_loss: 0.5739\n",
            "Epoch 104/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5589 - val_loss: 0.5742\n",
            "Epoch 105/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5561 - val_loss: 0.5731\n",
            "Epoch 106/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5543 - val_loss: 0.5770\n",
            "Epoch 107/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5523 - val_loss: 0.5753\n",
            "Epoch 108/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5536 - val_loss: 0.5749\n",
            "Epoch 109/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5506 - val_loss: 0.5790\n",
            "Epoch 110/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5501 - val_loss: 0.5757\n",
            "Epoch 111/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5494 - val_loss: 0.5802\n",
            "Epoch 112/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5483 - val_loss: 0.5833\n",
            "Epoch 113/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5472 - val_loss: 0.5822\n",
            "Epoch 114/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5454 - val_loss: 0.5821\n",
            "Epoch 115/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5458 - val_loss: 0.5814\n",
            "Epoch 116/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5432 - val_loss: 0.5799\n",
            "Epoch 117/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5433 - val_loss: 0.5845\n",
            "Epoch 118/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5402 - val_loss: 0.5848\n",
            "Epoch 119/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5412 - val_loss: 0.5845\n",
            "Epoch 120/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5366 - val_loss: 0.5895\n",
            "Epoch 121/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5390 - val_loss: 0.5831\n",
            "Epoch 122/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5366 - val_loss: 0.5892\n",
            "Epoch 123/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5377 - val_loss: 0.5911\n",
            "Epoch 124/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5343 - val_loss: 0.5928\n",
            "Epoch 125/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5344 - val_loss: 0.5878\n",
            "Epoch 126/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5325 - val_loss: 0.5914\n",
            "Epoch 127/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5317 - val_loss: 0.5937\n",
            "Epoch 128/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5310 - val_loss: 0.5955\n",
            "Epoch 129/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5297 - val_loss: 0.5948\n",
            "Epoch 130/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5281 - val_loss: 0.6000\n",
            "Epoch 131/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5260 - val_loss: 0.5917\n",
            "Epoch 132/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5260 - val_loss: 0.6007\n",
            "Epoch 133/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5239 - val_loss: 0.5969\n",
            "Epoch 134/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5253 - val_loss: 0.5990\n",
            "Epoch 135/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5216 - val_loss: 0.6029\n",
            "Epoch 136/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5232 - val_loss: 0.6037\n",
            "Epoch 137/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5208 - val_loss: 0.6044\n",
            "Epoch 138/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5200 - val_loss: 0.6039\n",
            "Epoch 139/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5208 - val_loss: 0.6034\n",
            "Epoch 140/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5168 - val_loss: 0.6074\n",
            "Epoch 141/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5168 - val_loss: 0.6070\n",
            "Epoch 142/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5151 - val_loss: 0.6057\n",
            "Epoch 143/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5156 - val_loss: 0.6121\n",
            "Epoch 144/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5148 - val_loss: 0.6119\n",
            "Epoch 145/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5111 - val_loss: 0.6093\n",
            "Epoch 146/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5100 - val_loss: 0.6098\n",
            "Epoch 147/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5100 - val_loss: 0.6127\n",
            "Epoch 148/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5079 - val_loss: 0.6150\n",
            "Epoch 149/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5073 - val_loss: 0.6135\n",
            "Epoch 150/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5074 - val_loss: 0.6133\n",
            "Epoch 151/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5054 - val_loss: 0.6125\n",
            "Epoch 152/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5058 - val_loss: 0.6177\n",
            "Epoch 153/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5041 - val_loss: 0.6190\n",
            "Epoch 154/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5014 - val_loss: 0.6237\n",
            "Epoch 155/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5008 - val_loss: 0.6231\n",
            "Epoch 156/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.4998 - val_loss: 0.6211\n",
            "Epoch 157/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.4982 - val_loss: 0.6211\n",
            "Epoch 158/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.4983 - val_loss: 0.6253\n",
            "Epoch 159/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.4977 - val_loss: 0.6213\n",
            "Epoch 160/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.4971 - val_loss: 0.6266\n",
            "Epoch 161/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.4949 - val_loss: 0.6268\n",
            "Epoch 162/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.4945 - val_loss: 0.6272\n",
            "Epoch 163/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.4931 - val_loss: 0.6314\n",
            "Epoch 164/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.4923 - val_loss: 0.6303\n",
            "Epoch 165/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.4915 - val_loss: 0.6253\n",
            "Epoch 166/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.4890 - val_loss: 0.6281\n",
            "Epoch 167/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.4896 - val_loss: 0.6285\n",
            "Epoch 168/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.4888 - val_loss: 0.6333\n",
            "Epoch 169/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.4871 - val_loss: 0.6319\n",
            "Epoch 170/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.4862 - val_loss: 0.6360\n",
            "Epoch 171/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.4851 - val_loss: 0.6320\n",
            "Epoch 172/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.4867 - val_loss: 0.6377\n",
            "Epoch 173/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5356 - val_loss: 0.6142\n",
            "Epoch 174/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5537 - val_loss: 0.6057\n",
            "Epoch 175/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5399 - val_loss: 0.6004\n",
            "Epoch 176/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5351 - val_loss: 0.5999\n",
            "Epoch 177/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5320 - val_loss: 0.6029\n",
            "Epoch 178/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5287 - val_loss: 0.6036\n",
            "Epoch 179/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5293 - val_loss: 0.5969\n",
            "Epoch 180/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5288 - val_loss: 0.6020\n",
            "Epoch 181/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5253 - val_loss: 0.6034\n",
            "Epoch 182/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5279 - val_loss: 0.5948\n",
            "Epoch 183/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5247 - val_loss: 0.6019\n",
            "Epoch 184/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5274 - val_loss: 0.6016\n",
            "Epoch 185/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5254 - val_loss: 0.6032\n",
            "Epoch 186/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5231 - val_loss: 0.6067\n",
            "Epoch 187/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5226 - val_loss: 0.6083\n",
            "Epoch 188/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5228 - val_loss: 0.6019\n",
            "Epoch 189/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5211 - val_loss: 0.6038\n",
            "Epoch 190/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5223 - val_loss: 0.6037\n",
            "Epoch 191/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5203 - val_loss: 0.6042\n",
            "Epoch 192/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5208 - val_loss: 0.6120\n",
            "Epoch 193/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5192 - val_loss: 0.6057\n",
            "Epoch 194/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5188 - val_loss: 0.6083\n",
            "Epoch 195/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5191 - val_loss: 0.6000\n",
            "Epoch 196/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5215 - val_loss: 0.6075\n",
            "Epoch 197/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5179 - val_loss: 0.6023\n",
            "Epoch 198/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5186 - val_loss: 0.6105\n",
            "Epoch 199/200\n",
            "350/350 [==============================] - 7s 19ms/step - loss: 0.5169 - val_loss: 0.6035\n",
            "Epoch 200/200\n",
            "350/350 [==============================] - 7s 20ms/step - loss: 0.5170 - val_loss: 0.6107\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x786cd8c6df30>"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "model.fit([encoder_input_data, decoder_input_data],  # training data\n",
        "          decoder_target_data,                       # labels (left shift of the target sequences)\n",
        "          batch_size=64, epochs=200, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('seq2seq_bilstm_rev.keras')"
      ],
      "metadata": {
        "id": "fhs7IhEqiPys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmZ3n18liPyt"
      },
      "source": [
        "#### c5) Make predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "aEZzWUHZiPyt"
      },
      "outputs": [],
      "source": [
        "# Reverse-lookup token index to decode sequences back to something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "y_3zEqA-iPyt"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=3)\n",
        "        # greedy selection\n",
        "        sampled_token_index = numpy.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        # multinomial sampling with temperature\n",
        "        #temperature=0.25\n",
        "        #pred = output_tokens[0, -1, :] ** (1.0 / temperature)\n",
        "        #temp_pred = pred / np.sum(pred)\n",
        "        #sampled_token_index = np.random.choice(len(output_tokens[0, -1, :]), p=temp_pred)\n",
        "\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index+1]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        states_value = [h, c]\n",
        "        decoded_sentence = decoded_sentence[::-1]\n",
        "    return decoded_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in range(500, 502):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    decoded_sentence = decoded_sentence.strip()\n",
        "    print('-')\n",
        "    print('English:       ', test_input_texts[seq_index])\n",
        "    print('Your target language (true): ', target_texts[seq_index][1:-1])\n",
        "    print('Your target language (pred): ', decoded_sentence[0:-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46fc4471-1cff-4b41-fccf-bd13c48639e5",
        "id": "5NCovyVDiPyt"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 615ms/step\n",
            "-\n",
            "English:        how are the kids\n",
            "Your target language (true):  זה פוגעני\n",
            "Your target language (pred):  אהם\t\tבלהיויש\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "-\n",
            "English:        come if you can\n",
            "Your target language (true):  זהו את תום\n",
            "Your target language (pred):  אהם\t\tבלהיויש\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THz9BRjDiPyt"
      },
      "source": [
        "#### c6) Translate an English sentence to the target language"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = 'I love you'\n",
        "\n",
        "input_tokens = [char for char in input_sentence]\n",
        "\n",
        "input_sequence = text2sequences_heb(max_encoder_seq_length, [input_tokens], lang=\"Eng\")[0]\n",
        "\n",
        "input_x = onehot_encode(input_sequence, max_encoder_seq_length, num_encoder_tokens)\n",
        "\n",
        "translated_sentence = decode_sequence(input_x)\n",
        "\n",
        "translated_sentence = translated_sentence.strip()\n",
        "\n",
        "print('source sentence is: ' + input_sentence)\n",
        "print('translated sentence is: ' + translated_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4068f639-e21f-4a8e-fbf4-f362ca27ba16",
        "id": "BrcvKHA1iPyt"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 19ms/step\n",
            "source sentence is: I love you\n",
            "translated sentence is: אםם\t\tב\tהיויהכ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRk8D8wCiPyt"
      },
      "source": [
        "#### c7) Evaluate the translation using BLEU score (for 5 samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "individual_bleu_scores = []\n",
        "\n",
        "for seq_index in range(5000, 5005):\n",
        "\n",
        "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    decoded_sentence = decoded_sentence.strip()\n",
        "    actual_sentence = test_target_texts[seq_index][1:-1]\n",
        "    print(\"Decoded sentence: \", decoded_sentence)\n",
        "    print(\"Actual sentence: \", actual_sentence)\n",
        "    candidate = decoded_sentence.split()\n",
        "    references = actual_sentence.split()\n",
        "    # Calculate BLEU score for this sentence\n",
        "    bleu_score = sentence_bleu(references, candidate, weights=(1, 0, 0, 0))\n",
        "    print('Cumulative 1-gram: %f' % sentence_bleu(references, candidate, weights=(1, 0, 0, 0)))\n",
        "    print('Cumulative 2-gram: %f' % sentence_bleu(references, candidate, weights=(0.5, 0.5, 0, 0)))\n",
        "    print('Cumulative 3-gram: %f' % sentence_bleu(references, candidate, weights=(0.33, 0.33, 0.33, 0)))\n",
        "    print('Cumulative 4-gram: %f' % sentence_bleu(references, candidate, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "    individual_bleu_scores.append(bleu_score)\n",
        "\n",
        "# Calculate the average BLEU score\n",
        "average_bleu_score = sum(individual_bleu_scores) / len(individual_bleu_scores)\n",
        "\n",
        "print(\"Average BLEU Score on the Test Set:\", average_bleu_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0134a83e-92b9-44dc-975a-fa248f0cb8ee",
        "id": "tfcSRtW2iPyt"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "Decoded sentence:  אהם\t\tבלהיוישכ\n",
            "Actual sentence:  תום תמיד משקר\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Decoded sentence:  אהם\t\tב\tהיוישמ\n",
            "Actual sentence:  אתה יכול להביא את זה\n",
            "Cumulative 1-gram: 0.333333\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Decoded sentence:  שךמש\t\tת\tויוישיהא\n",
            "Actual sentence:  אני אוהב אותם\n",
            "Cumulative 1-gram: 0.333333\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Decoded sentence:  אהם\t\tבלהיויהכ\n",
            "Actual sentence:  לעולם אינני דואג\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Decoded sentence:  אהם\t\tבלהיוישכ\n",
            "Actual sentence:  אנו לא מוכנות\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "Average BLEU Score on the Test Set: 0.13333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZbPJ7LTnoYm"
      },
      "source": [
        "### d) LSTM Seq2Seq model with Attention\n",
        "\n",
        "\n",
        "(Source: https://saturncloud.io/blog/add-attention-mechanism-to-an-lstm-model-in-keras/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r3XKqLSnoYn"
      },
      "source": [
        "#### d1) Encoder network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "K4WuJFSYnoYn"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input, LSTM\n",
        "from keras.models import Model\n",
        "\n",
        "latent_dim = 256\n",
        "\n",
        "# inputs of the encoder network\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens),\n",
        "                       name='encoder_inputs')\n",
        "\n",
        "# set the LSTM layer\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True,\n",
        "                    dropout=0.2, name='encoder_lstm')\n",
        "encoder_lstm_output, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "\n",
        "# build the encoder network model\n",
        "encoder_model = Model(inputs=encoder_inputs,\n",
        "                      outputs=[state_h, state_c],\n",
        "                      name='encoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08c333e4-f170-4c6f-f7c0-e40c41a6429b",
        "id": "6T-k3g3knoYo"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_inputs (InputLayer  [(None, None, 28)]        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " encoder_lstm (LSTM)         [(None, 256),             291840    \n",
            "                              (None, 256),                       \n",
            "                              (None, 256)]                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 291840 (1.11 MB)\n",
            "Trainable params: 291840 (1.11 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "encoder_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqsSRC46noYo"
      },
      "source": [
        "#### d2) Decoder network with Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "JDXQ4PytALBY"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Attention, Concatenate\n",
        "\n",
        "\n",
        "# inputs of the decoder network\n",
        "decoder_input_h = Input(shape=(latent_dim,), name='decoder_input_h')\n",
        "decoder_input_c = Input(shape=(latent_dim,), name='decoder_input_c')\n",
        "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
        "\n",
        "\n",
        "# Decoder LSTM\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True,\n",
        "                    return_state=True, dropout=0.2, name='decoder_lstm')\n",
        "decoder_lstm_outputs, state_h, state_c = decoder_lstm(decoder_input_x,\n",
        "                                                      initial_state=[decoder_input_h, decoder_input_c])\n",
        "\n",
        "# Attention layer\n",
        "attention = Attention()\n",
        "attention_output = attention([decoder_lstm_outputs, decoder_input_h])\n",
        "\n",
        "\n",
        "# set the dense layer\n",
        "decoder_dense_with_attention = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense_with_attention')\n",
        "decoder_outputs_with_attention = decoder_dense_with_attention(attention_output)\n",
        "\n",
        "# build the decoder network model\n",
        "decoder_model = Model(inputs=[decoder_input_x, decoder_input_h, decoder_input_c],\n",
        "                      outputs=[decoder_outputs_with_attention, state_h, state_c],\n",
        "                      name='decoder')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9NaAEybGnAk",
        "outputId": "8cb9c988-85c1-4e80-8f9c-5b28b0ee8546"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " decoder_input_x (InputLaye  [(None, None, 47)]           0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " decoder_input_h (InputLaye  [(None, 256)]                0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " decoder_input_c (InputLaye  [(None, 256)]                0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " decoder_lstm (LSTM)         [(None, None, 256),          311296    ['decoder_input_x[0][0]',     \n",
            "                              (None, 256),                           'decoder_input_h[0][0]',     \n",
            "                              (None, 256)]                           'decoder_input_c[0][0]']     \n",
            "                                                                                                  \n",
            " attention_1 (Attention)     (None, None, 256)            0         ['decoder_lstm[0][0]',        \n",
            "                                                                     'decoder_input_h[0][0]']     \n",
            "                                                                                                  \n",
            " decoder_dense_with_attenti  (None, None, 47)             12079     ['attention_1[0][0]']         \n",
            " on (Dense)                                                                                       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 323375 (1.23 MB)\n",
            "Trainable params: 323375 (1.23 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmE8y47vG8Bs"
      },
      "source": [
        "#### d3) Connect the encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "AZTgwgT3G8Bs"
      },
      "outputs": [],
      "source": [
        "# input layers\n",
        "encoder_input_x = Input(shape=(None, num_encoder_tokens), name='encoder_input_x')\n",
        "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
        "\n",
        "# connect encoder to decoder\n",
        "encoder_final_states = encoder_model([encoder_input_x])\n",
        "decoder_lstm_output, _, _ = decoder_lstm(decoder_input_x, initial_state=encoder_final_states)\n",
        "decoder_pred = decoder_dense_with_attention(decoder_lstm_output)\n",
        "\n",
        "model = Model(inputs=[encoder_input_x, decoder_input_x],\n",
        "              outputs=decoder_pred,\n",
        "              name='model_training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39432873-2ea4-49bd-85dd-42546157eb56",
        "id": "QglouFQvG8Bs"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.float32, name=None), name='decoder_lstm/PartitionedCall:2', description=\"created by layer 'decoder_lstm'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.float32, name='decoder_input_h'), name='decoder_input_h', description=\"created by layer 'decoder_input_h'\")\n"
          ]
        }
      ],
      "source": [
        "print(state_h)\n",
        "print(decoder_input_h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0465fbec-8b60-468b-8207-afab8c0b78b4",
        "id": "srrjudt6G8Bt"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_training\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_input_x (InputLaye  [(None, None, 28)]           0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " decoder_input_x (InputLaye  [(None, None, 47)]           0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " encoder (Functional)        [(None, 256),                291840    ['encoder_input_x[0][0]']     \n",
            "                              (None, 256)]                                                        \n",
            "                                                                                                  \n",
            " decoder_lstm (LSTM)         [(None, None, 256),          311296    ['decoder_input_x[0][0]',     \n",
            "                              (None, 256),                           'encoder[0][0]',             \n",
            "                              (None, 256)]                           'encoder[0][1]']             \n",
            "                                                                                                  \n",
            " decoder_dense_with_attenti  (None, None, 47)             12079     ['decoder_lstm[1][0]']        \n",
            " on (Dense)                                                                                       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 615215 (2.35 MB)\n",
            "Trainable params: 615215 (2.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC5lrG0UG8Bt"
      },
      "source": [
        "#### d4) Fit the model on the bilingual dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ad57e48-833c-440f-e2b7-93af58862136",
        "id": "eTlgPJGeG8Bt"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of encoder_input_data(28000, 20, 28)\n",
            "shape of decoder_input_data(28000, 44, 47)\n",
            "shape of decoder_target_data(28000, 44, 47)\n"
          ]
        }
      ],
      "source": [
        "print('shape of encoder_input_data' + str(encoder_input_data.shape))\n",
        "print('shape of decoder_input_data' + str(decoder_input_data.shape))\n",
        "print('shape of decoder_target_data' + str(decoder_target_data.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab9ccbc5-9154-4b40-904c-83f56bc035e9",
        "id": "yk8AhpH3G8Bt"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "700/700 [==============================] - 9s 8ms/step - loss: 0.9234 - val_loss: 0.7689\n",
            "Epoch 2/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.7786 - val_loss: 0.7269\n",
            "Epoch 3/200\n",
            "700/700 [==============================] - 5s 6ms/step - loss: 0.7482 - val_loss: 0.7002\n",
            "Epoch 4/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.7260 - val_loss: 0.6758\n",
            "Epoch 5/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.7077 - val_loss: 0.6579\n",
            "Epoch 6/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.6925 - val_loss: 0.6426\n",
            "Epoch 7/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.6810 - val_loss: 0.6284\n",
            "Epoch 8/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.6703 - val_loss: 0.6182\n",
            "Epoch 9/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.6622 - val_loss: 0.6077\n",
            "Epoch 10/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.6546 - val_loss: 0.6011\n",
            "Epoch 11/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.6469 - val_loss: 0.5931\n",
            "Epoch 12/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.6414 - val_loss: 0.5891\n",
            "Epoch 13/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.6352 - val_loss: 0.5823\n",
            "Epoch 14/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.6295 - val_loss: 0.5773\n",
            "Epoch 15/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.6243 - val_loss: 0.5712\n",
            "Epoch 16/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.6209 - val_loss: 0.5689\n",
            "Epoch 17/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.6164 - val_loss: 0.5618\n",
            "Epoch 18/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.6122 - val_loss: 0.5598\n",
            "Epoch 19/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.6100 - val_loss: 0.5549\n",
            "Epoch 20/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.6044 - val_loss: 0.5526\n",
            "Epoch 21/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.6017 - val_loss: 0.5479\n",
            "Epoch 22/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5983 - val_loss: 0.5465\n",
            "Epoch 23/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5963 - val_loss: 0.5443\n",
            "Epoch 24/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5932 - val_loss: 0.5406\n",
            "Epoch 25/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5900 - val_loss: 0.5409\n",
            "Epoch 26/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5874 - val_loss: 0.5374\n",
            "Epoch 27/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5844 - val_loss: 0.5355\n",
            "Epoch 28/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5816 - val_loss: 0.5340\n",
            "Epoch 29/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5800 - val_loss: 0.5308\n",
            "Epoch 30/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5771 - val_loss: 0.5294\n",
            "Epoch 31/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5743 - val_loss: 0.5282\n",
            "Epoch 32/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5736 - val_loss: 0.5275\n",
            "Epoch 33/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5714 - val_loss: 0.5245\n",
            "Epoch 34/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5703 - val_loss: 0.5231\n",
            "Epoch 35/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5674 - val_loss: 0.5233\n",
            "Epoch 36/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5663 - val_loss: 0.5220\n",
            "Epoch 37/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5637 - val_loss: 0.5206\n",
            "Epoch 38/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5623 - val_loss: 0.5187\n",
            "Epoch 39/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5605 - val_loss: 0.5189\n",
            "Epoch 40/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5597 - val_loss: 0.5170\n",
            "Epoch 41/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5572 - val_loss: 0.5163\n",
            "Epoch 42/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5549 - val_loss: 0.5156\n",
            "Epoch 43/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5550 - val_loss: 0.5139\n",
            "Epoch 44/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5531 - val_loss: 0.5138\n",
            "Epoch 45/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5516 - val_loss: 0.5121\n",
            "Epoch 46/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5492 - val_loss: 0.5114\n",
            "Epoch 47/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5485 - val_loss: 0.5116\n",
            "Epoch 48/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5478 - val_loss: 0.5109\n",
            "Epoch 49/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5466 - val_loss: 0.5093\n",
            "Epoch 50/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5445 - val_loss: 0.5093\n",
            "Epoch 51/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5429 - val_loss: 0.5098\n",
            "Epoch 52/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5416 - val_loss: 0.5084\n",
            "Epoch 53/200\n",
            "700/700 [==============================] - 5s 6ms/step - loss: 0.5406 - val_loss: 0.5075\n",
            "Epoch 54/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5389 - val_loss: 0.5075\n",
            "Epoch 55/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5387 - val_loss: 0.5064\n",
            "Epoch 56/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5377 - val_loss: 0.5055\n",
            "Epoch 57/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5364 - val_loss: 0.5064\n",
            "Epoch 58/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5351 - val_loss: 0.5058\n",
            "Epoch 59/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5330 - val_loss: 0.5059\n",
            "Epoch 60/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.5322 - val_loss: 0.5047\n",
            "Epoch 61/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5314 - val_loss: 0.5046\n",
            "Epoch 62/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5306 - val_loss: 0.5034\n",
            "Epoch 63/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5304 - val_loss: 0.5045\n",
            "Epoch 64/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5294 - val_loss: 0.5058\n",
            "Epoch 65/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5287 - val_loss: 0.5041\n",
            "Epoch 66/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5272 - val_loss: 0.5036\n",
            "Epoch 67/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5255 - val_loss: 0.5024\n",
            "Epoch 68/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5263 - val_loss: 0.5033\n",
            "Epoch 69/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5247 - val_loss: 0.5031\n",
            "Epoch 70/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5233 - val_loss: 0.5028\n",
            "Epoch 71/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5224 - val_loss: 0.5023\n",
            "Epoch 72/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5219 - val_loss: 0.5009\n",
            "Epoch 73/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5206 - val_loss: 0.5012\n",
            "Epoch 74/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5218 - val_loss: 0.5014\n",
            "Epoch 75/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5193 - val_loss: 0.5012\n",
            "Epoch 76/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5182 - val_loss: 0.5007\n",
            "Epoch 77/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5189 - val_loss: 0.4997\n",
            "Epoch 78/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5184 - val_loss: 0.5007\n",
            "Epoch 79/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5151 - val_loss: 0.5010\n",
            "Epoch 80/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5161 - val_loss: 0.5005\n",
            "Epoch 81/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5151 - val_loss: 0.4997\n",
            "Epoch 82/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5137 - val_loss: 0.5007\n",
            "Epoch 83/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5140 - val_loss: 0.4997\n",
            "Epoch 84/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5128 - val_loss: 0.5000\n",
            "Epoch 85/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5118 - val_loss: 0.5003\n",
            "Epoch 86/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5119 - val_loss: 0.5005\n",
            "Epoch 87/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5099 - val_loss: 0.4987\n",
            "Epoch 88/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5092 - val_loss: 0.5002\n",
            "Epoch 89/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5102 - val_loss: 0.5001\n",
            "Epoch 90/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5082 - val_loss: 0.5003\n",
            "Epoch 91/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5080 - val_loss: 0.4991\n",
            "Epoch 92/200\n",
            "700/700 [==============================] - 5s 6ms/step - loss: 0.5079 - val_loss: 0.4995\n",
            "Epoch 93/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5071 - val_loss: 0.4994\n",
            "Epoch 94/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5065 - val_loss: 0.5002\n",
            "Epoch 95/200\n",
            "700/700 [==============================] - 5s 6ms/step - loss: 0.5050 - val_loss: 0.5017\n",
            "Epoch 96/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5054 - val_loss: 0.4997\n",
            "Epoch 97/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5050 - val_loss: 0.4995\n",
            "Epoch 98/200\n",
            "700/700 [==============================] - 5s 6ms/step - loss: 0.5034 - val_loss: 0.5000\n",
            "Epoch 99/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5032 - val_loss: 0.5005\n",
            "Epoch 100/200\n",
            "700/700 [==============================] - 5s 6ms/step - loss: 0.5021 - val_loss: 0.4990\n",
            "Epoch 101/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5017 - val_loss: 0.5000\n",
            "Epoch 102/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5020 - val_loss: 0.4986\n",
            "Epoch 103/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5004 - val_loss: 0.5013\n",
            "Epoch 104/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5005 - val_loss: 0.4993\n",
            "Epoch 105/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5005 - val_loss: 0.5005\n",
            "Epoch 106/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.5007 - val_loss: 0.4994\n",
            "Epoch 107/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.4992 - val_loss: 0.5005\n",
            "Epoch 108/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.4994 - val_loss: 0.4996\n",
            "Epoch 109/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.4979 - val_loss: 0.4999\n",
            "Epoch 110/200\n",
            "700/700 [==============================] - 4s 6ms/step - loss: 0.4958 - val_loss: 0.5008\n",
            "Epoch 111/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4974 - val_loss: 0.4998\n",
            "Epoch 112/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4965 - val_loss: 0.5015\n",
            "Epoch 113/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4955 - val_loss: 0.5002\n",
            "Epoch 114/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4972 - val_loss: 0.5002\n",
            "Epoch 115/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4963 - val_loss: 0.5011\n",
            "Epoch 116/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4954 - val_loss: 0.5008\n",
            "Epoch 117/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4952 - val_loss: 0.5012\n",
            "Epoch 118/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4955 - val_loss: 0.5015\n",
            "Epoch 119/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4928 - val_loss: 0.5009\n",
            "Epoch 120/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4946 - val_loss: 0.5011\n",
            "Epoch 121/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4934 - val_loss: 0.5006\n",
            "Epoch 122/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4918 - val_loss: 0.5006\n",
            "Epoch 123/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4912 - val_loss: 0.5020\n",
            "Epoch 124/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4912 - val_loss: 0.5020\n",
            "Epoch 125/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4925 - val_loss: 0.5015\n",
            "Epoch 126/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4911 - val_loss: 0.5016\n",
            "Epoch 127/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4914 - val_loss: 0.5008\n",
            "Epoch 128/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4895 - val_loss: 0.5037\n",
            "Epoch 129/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4895 - val_loss: 0.5026\n",
            "Epoch 130/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4895 - val_loss: 0.5026\n",
            "Epoch 131/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4884 - val_loss: 0.5007\n",
            "Epoch 132/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4887 - val_loss: 0.5001\n",
            "Epoch 133/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4875 - val_loss: 0.5019\n",
            "Epoch 134/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4872 - val_loss: 0.5012\n",
            "Epoch 135/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4880 - val_loss: 0.5005\n",
            "Epoch 136/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4868 - val_loss: 0.5023\n",
            "Epoch 137/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4874 - val_loss: 0.5023\n",
            "Epoch 138/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4859 - val_loss: 0.5026\n",
            "Epoch 139/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4865 - val_loss: 0.5014\n",
            "Epoch 140/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4852 - val_loss: 0.5034\n",
            "Epoch 141/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4854 - val_loss: 0.5024\n",
            "Epoch 142/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4850 - val_loss: 0.5015\n",
            "Epoch 143/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4848 - val_loss: 0.5017\n",
            "Epoch 144/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4847 - val_loss: 0.5018\n",
            "Epoch 145/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4836 - val_loss: 0.5025\n",
            "Epoch 146/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4835 - val_loss: 0.5030\n",
            "Epoch 147/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4837 - val_loss: 0.5038\n",
            "Epoch 148/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4827 - val_loss: 0.5024\n",
            "Epoch 149/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4831 - val_loss: 0.5022\n",
            "Epoch 150/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4838 - val_loss: 0.5027\n",
            "Epoch 151/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4820 - val_loss: 0.5047\n",
            "Epoch 152/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4815 - val_loss: 0.5033\n",
            "Epoch 153/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4823 - val_loss: 0.5033\n",
            "Epoch 154/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4812 - val_loss: 0.5002\n",
            "Epoch 155/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4825 - val_loss: 0.5029\n",
            "Epoch 156/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4817 - val_loss: 0.5047\n",
            "Epoch 157/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4815 - val_loss: 0.5029\n",
            "Epoch 158/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4803 - val_loss: 0.5039\n",
            "Epoch 159/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4793 - val_loss: 0.5030\n",
            "Epoch 160/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4798 - val_loss: 0.5044\n",
            "Epoch 161/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4812 - val_loss: 0.5040\n",
            "Epoch 162/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4796 - val_loss: 0.5035\n",
            "Epoch 163/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4784 - val_loss: 0.5037\n",
            "Epoch 164/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4784 - val_loss: 0.5031\n",
            "Epoch 165/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4787 - val_loss: 0.5027\n",
            "Epoch 166/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4789 - val_loss: 0.5035\n",
            "Epoch 167/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4785 - val_loss: 0.5050\n",
            "Epoch 168/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4772 - val_loss: 0.5045\n",
            "Epoch 169/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4783 - val_loss: 0.5046\n",
            "Epoch 170/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4786 - val_loss: 0.5054\n",
            "Epoch 171/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4768 - val_loss: 0.5046\n",
            "Epoch 172/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4762 - val_loss: 0.5061\n",
            "Epoch 173/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4776 - val_loss: 0.5036\n",
            "Epoch 174/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4759 - val_loss: 0.5045\n",
            "Epoch 175/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4759 - val_loss: 0.5039\n",
            "Epoch 176/200\n",
            "700/700 [==============================] - 5s 6ms/step - loss: 0.4758 - val_loss: 0.5058\n",
            "Epoch 177/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4765 - val_loss: 0.5076\n",
            "Epoch 178/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4756 - val_loss: 0.5057\n",
            "Epoch 179/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4745 - val_loss: 0.5046\n",
            "Epoch 180/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4743 - val_loss: 0.5052\n",
            "Epoch 181/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4742 - val_loss: 0.5052\n",
            "Epoch 182/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4756 - val_loss: 0.5046\n",
            "Epoch 183/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4749 - val_loss: 0.5082\n",
            "Epoch 184/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4756 - val_loss: 0.5062\n",
            "Epoch 185/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4742 - val_loss: 0.5063\n",
            "Epoch 186/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4751 - val_loss: 0.5068\n",
            "Epoch 187/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4742 - val_loss: 0.5067\n",
            "Epoch 188/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4740 - val_loss: 0.5072\n",
            "Epoch 189/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4721 - val_loss: 0.5078\n",
            "Epoch 190/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4732 - val_loss: 0.5059\n",
            "Epoch 191/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4741 - val_loss: 0.5062\n",
            "Epoch 192/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4716 - val_loss: 0.5073\n",
            "Epoch 193/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4732 - val_loss: 0.5060\n",
            "Epoch 194/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4721 - val_loss: 0.5069\n",
            "Epoch 195/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4713 - val_loss: 0.5076\n",
            "Epoch 196/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4713 - val_loss: 0.5062\n",
            "Epoch 197/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4718 - val_loss: 0.5091\n",
            "Epoch 198/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4717 - val_loss: 0.5066\n",
            "Epoch 199/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4726 - val_loss: 0.5077\n",
            "Epoch 200/200\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.4721 - val_loss: 0.5081\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x786cd777eb30>"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ],
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "\n",
        "model.fit([encoder_input_data, decoder_input_data],  # training data\n",
        "          decoder_target_data,                       # labels (left shift of the target sequences)\n",
        "          batch_size=32, epochs=200, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEjovkMZG8Bu"
      },
      "outputs": [],
      "source": [
        "model.save('seq2seq_attention.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjttqWpJG8Bu"
      },
      "source": [
        "#### d5) Make predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "hoCy3aHKG8Bu"
      },
      "outputs": [],
      "source": [
        "# Reverse-lookup token index to decode sequences back to something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "7MyjnGmTG8Bu"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=3)\n",
        "        # greedy selection\n",
        "        # sampled_token_index = numpy.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        # multinomial sampling with temperature\n",
        "        temperature=0.2\n",
        "        pred = output_tokens[0, -1, :] ** (1.0 / temperature)\n",
        "        temp_pred = pred / np.sum(pred)\n",
        "        sampled_token_index = np.random.choice(len(output_tokens[0, -1, :]), p=temp_pred)\n",
        "\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index+1]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        states_value = [h, c]\n",
        "        decoded_sentence = decoded_sentence[::-1]\n",
        "\n",
        "    return decoded_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d762e4c2-944c-4ef3-e370-0c9b1811aaf3",
        "id": "6Vtk-zjWG8Bu"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 319ms/step\n",
            "-\n",
            "English:        enjoy the game\n",
            "Your target language (true):  אתה יכול לטלפן לו\n",
            "Your target language (pred):  א\tחניתי\tדתלוע\tח\t\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "-\n",
            "English:        ill do anything\n",
            "Your target language (true):  אל תחששי לי\n",
            "Your target language (pred):  עימפי\tדתלוהק\n"
          ]
        }
      ],
      "source": [
        "for seq_index in range(5100, 5102):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    decoded_sentence = decoded_sentence.strip()\n",
        "    print('-')\n",
        "    print('English:       ', test_input_texts[seq_index])\n",
        "    print('Your target language (true): ', target_texts[seq_index][1:-1])\n",
        "    print('Your target language (pred): ', decoded_sentence[0:-1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVaJM6yQG8Bu"
      },
      "source": [
        "#### d6) Translate an English sentence to the target language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0ac18e6-e7fd-47ea-bbc4-09c77cb8740f",
        "id": "I_g-8tjQG8Bv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n",
            "source sentence is: I love you\n",
            "translated sentence is: אןותיתיתומולוע\n"
          ]
        }
      ],
      "source": [
        "input_sentence = 'I love you'\n",
        "\n",
        "input_tokens = [char for char in input_sentence]\n",
        "\n",
        "input_sequence = text2sequences_heb(max_encoder_seq_length, [input_tokens], lang=\"Eng\")[0]\n",
        "\n",
        "input_x = onehot_encode(input_sequence, max_encoder_seq_length, num_encoder_tokens)\n",
        "\n",
        "translated_sentence = decode_sequence(input_x)\n",
        "translated_sentence = translated_sentence.strip()\n",
        "\n",
        "print('source sentence is: ' + input_sentence)\n",
        "print('translated sentence is: ' + translated_sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QodjEV2-G8Bv"
      },
      "source": [
        "#### d7) Evaluate the translation using BLEU score (5 samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "694215fb-e593-4868-a11e-febb816e17e2",
        "id": "1c6zTdsnG8Bv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 17ms/step\n",
            "Decoded sentence:  אלהםיורתתיויורוא\n",
            "Actual sentence:  טום היה חמקמק\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Decoded sentence:  א\tי\tרתל\tטכג\n",
            "Actual sentence:  תום לא רגיש\n",
            "Cumulative 1-gram: 0.500000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "Decoded sentence:  עימפי\tדתלוהקא\n",
            "Actual sentence:  תום קרץ בחזרה\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Decoded sentence:  אתי\tרת\tויתלא\n",
            "Actual sentence:  אני לא סובל זוחלים\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "Decoded sentence:  א\tהזיורתתויתהליש\n",
            "Actual sentence:  איפה הספר\n",
            "Cumulative 1-gram: 0.183940\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "Average BLEU Score on the Test Set: 0.13678794411714423\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "individual_bleu_scores = []\n",
        "\n",
        "for seq_index in range(4000, 4005):\n",
        "\n",
        "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    decoded_sentence = decoded_sentence.strip()\n",
        "    actual_sentence = test_target_texts[seq_index][1:-1]\n",
        "    print(\"Decoded sentence: \", decoded_sentence)\n",
        "    print(\"Actual sentence: \", actual_sentence)\n",
        "    candidate = decoded_sentence.split()\n",
        "    references = actual_sentence.split()\n",
        "    # Calculate BLEU score for this sentence\n",
        "    bleu_score = sentence_bleu(references, candidate, weights=(1, 0, 0, 0))\n",
        "    print('Cumulative 1-gram: %f' % sentence_bleu(references, candidate, weights=(1, 0, 0, 0)))\n",
        "    print('Cumulative 2-gram: %f' % sentence_bleu(references, candidate, weights=(0.5, 0.5, 0, 0)))\n",
        "    print('Cumulative 3-gram: %f' % sentence_bleu(references, candidate, weights=(0.33, 0.33, 0.33, 0)))\n",
        "    print('Cumulative 4-gram: %f' % sentence_bleu(references, candidate, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "    individual_bleu_scores.append(bleu_score)\n",
        "\n",
        "# Calculate the average BLEU score\n",
        "average_bleu_score = sum(individual_bleu_scores) / len(individual_bleu_scores)\n",
        "\n",
        "print(\"Average BLEU Score on the Test Set:\", average_bleu_score)\n"
      ]
    }
  ]
}